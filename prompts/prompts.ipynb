{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_section",
            "metadata": {},
            "source": [
                "# Prompts in LangChain\n",
                "\n",
                "## What are Prompts?\n",
                "\n",
                "**Prompts** are the instructions you give to a language model to get desired outputs. In LangChain, prompts are **structured templates** that allow you to create dynamic, reusable instructions instead of hardcoding text.\n",
                "\n",
                "### Why Use Prompt Templates?\n",
                "\n",
                "1. **Reusability**: Write once, use with different inputs\n",
                "2. **Consistency**: Maintain consistent formatting across your application\n",
                "3. **Maintainability**: Update prompts in one place\n",
                "4. **Validation**: LangChain validates that all required variables are provided\n",
                "5. **Composability**: Combine prompts with models and parsers seamlessly\n",
                "\n",
                "### Prompts in the LangChain Pipeline\n",
                "\n",
                "```\n",
                "Input Data → Prompt Template → Formatted Prompt → Model → Output\n",
                "```\n",
                "\n",
                "### Anatomy of a Good Prompt\n",
                "\n",
                "1. **Instruction**: What you want the model to do\n",
                "2. **Context**: Background information\n",
                "3. **Input Data**: The specific data to process\n",
                "4. **Output Format**: How you want the response structured\n",
                "\n",
                "### Key Principle\n",
                "\n",
                "**Better prompts = Better outputs**. Prompt engineering is the art of crafting effective instructions to get the best results from LLMs."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompt_template_intro",
            "metadata": {},
            "source": [
                "## PromptTemplate\n",
                "\n",
                "### Definition\n",
                "\n",
                "`PromptTemplate` is the simplest and most commonly used prompt class in LangChain. It creates a template with **placeholders** that get filled with actual values at runtime.\n",
                "\n",
                "### Key Features\n",
                "\n",
                "- **Variable Substitution**: Use `{variable_name}` as placeholders\n",
                "- **Type Safety**: Validates that all required variables are provided\n",
                "- **String-Based**: Works with simple text completion models\n",
                "- **Composable**: Can be chained with models and parsers\n",
                "\n",
                "### Use Cases\n",
                "\n",
                "- Simple text generation tasks\n",
                "- Single-turn completions\n",
                "- Parameterized queries\n",
                "- Content generation with templates\n",
                "\n",
                "### Basic Syntax\n",
                "\n",
                "```python\n",
                "PromptTemplate(\n",
                "    template=\"Your template with {variable}\",\n",
                "    input_variables=[\"variable\"]\n",
                ")\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prompt_template_example",
            "metadata": {},
            "outputs": [],
            "source": [
                "# PromptTemplate Example\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load API keys\n",
                "load_dotenv()\n",
                "\n",
                "# Create model\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3\n",
                ")\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "# Create prompt template\n",
                "prompt = PromptTemplate(\n",
                "    template=\"Generate few points on {topic}.\",\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "# Chain prompt with model using LCEL\n",
                "chain = prompt | model\n",
                "\n",
                "# Invoke with actual value\n",
                "result = chain.invoke({'topic': 'AI'})\n",
                "print(result.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompt_template_explanation",
            "metadata": {},
            "source": [
                "### Code Explanation:\n",
                "\n",
                "1. **Import PromptTemplate**: From `langchain_core.prompts`\n",
                "2. **Setup model**: Create a chat model (HuggingFace in this example)\n",
                "3. **Create template**:\n",
                "   - `template`: String with `{topic}` placeholder\n",
                "   - `input_variables`: List of variable names (must match placeholders)\n",
                "4. **Chain with LCEL**: `prompt | model` creates a pipeline\n",
                "   - Data flows: input dict → prompt (formats) → model (generates)\n",
                "5. **Invoke**: Pass dictionary with actual values\n",
                "   - `{'topic': 'AI'}` → \"Generate few points on AI.\"\n",
                "6. **Get result**: `result.content` contains the generated text\n",
                "\n",
                "### Advanced Template Examples:\n",
                "\n",
                "```python\n",
                "# Multiple variables\n",
                "prompt = PromptTemplate(\n",
                "    template=\"Write a {length} {style} story about {topic}\",\n",
                "    input_variables=[\"length\", \"style\", \"topic\"]\n",
                ")\n",
                "\n",
                "# Multi-line template\n",
                "prompt = PromptTemplate(\n",
                "    template=\"\"\"\n",
                "    You are a {role}.\n",
                "    Task: {task}\n",
                "    Context: {context}\n",
                "    \n",
                "    Please provide a detailed response.\n",
                "    \"\"\",\n",
                "    input_variables=[\"role\", \"task\", \"context\"]\n",
                ")\n",
                "\n",
                "# From template string (shorthand)\n",
                "prompt = PromptTemplate.from_template(\n",
                "    \"Summarize this text: {text}\"\n",
                ")\n",
                "```\n",
                "\n",
                "### Best Practices:\n",
                "\n",
                "✅ **DO:**\n",
                "- Use descriptive variable names\n",
                "- Include clear instructions\n",
                "- Specify output format\n",
                "- Test with different inputs\n",
                "\n",
                "❌ **DON'T:**\n",
                "- Use ambiguous placeholders\n",
                "- Forget to list all variables\n",
                "- Make prompts too complex\n",
                "- Hardcode values that should be variables"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "chat_prompt_intro",
            "metadata": {},
            "source": [
                "## ChatPromptTemplate\n",
                "\n",
                "### Definition\n",
                "\n",
                "`ChatPromptTemplate` is designed for **conversational AI** applications. It structures prompts as a sequence of messages with different roles (System, Human, AI).\n",
                "\n",
                "### Key Features\n",
                "\n",
                "- **Role-Based Messages**: System, Human, AI messages\n",
                "- **Conversation Structure**: Natural chat format\n",
                "- **Memory Support**: Can include conversation history\n",
                "- **System Instructions**: Persistent behavior guidelines\n",
                "\n",
                "### Message Roles\n",
                "\n",
                "| Role | Purpose | Example |\n",
                "|------|---------|----------|\n",
                "| **System** | Set AI behavior/persona | \"You are a helpful assistant\" |\n",
                "| **Human** | User input | \"What is the capital of France?\" |\n",
                "| **AI** | Model response | \"The capital of France is Paris\" |\n",
                "\n",
                "### Use Cases\n",
                "\n",
                "- Chatbots and assistants\n",
                "- Multi-turn conversations\n",
                "- Context-aware responses\n",
                "- Customer support bots\n",
                "- Interactive applications\n",
                "\n",
                "### Why Use ChatPromptTemplate?\n",
                "\n",
                "Modern chat models (GPT-4, Claude, Llama) are **instruction-tuned** and work better with structured messages than plain text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "chat_prompt_example",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ChatPromptTemplate with Memory Example\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "# Create model\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3\n",
                ")\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "# Create chat prompt with system message and memory\n",
                "prompt = ChatPromptTemplate([\n",
                "    ('system', '''You are a Helpful AI assistant. Your role is to give short and \n",
                "     straight-to-point, direct, concise responses.\n",
                "     Format: give your answer and then stop.'''),\n",
                "    MessagesPlaceholder(variable_name='memory'),\n",
                "    ('human', '{query}')\n",
                "])\n",
                "\n",
                "# Memory to store conversation history\n",
                "memory = []\n",
                "\n",
                "# Chat loop\n",
                "while True:\n",
                "    query = input('-->You: ')\n",
                "    if query.lower() == 'exit':\n",
                "        break\n",
                "    \n",
                "    # Create chain and invoke\n",
                "    chain = prompt | model\n",
                "    response = chain.invoke({'query': query, 'memory': memory})\n",
                "    \n",
                "    print('-->AI:', response.content)\n",
                "    \n",
                "    # Store in memory\n",
                "    memory.append(HumanMessage(content=query))\n",
                "    memory.append(AIMessage(content=response.content))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "chat_prompt_explanation",
            "metadata": {},
            "source": [
                "### Code Explanation:\n",
                "\n",
                "1. **Import message classes**: `HumanMessage`, `AIMessage` for conversation history\n",
                "2. **Create ChatPromptTemplate** with three parts:\n",
                "   - **System message** `('system', '...')`: Sets AI behavior (persistent)\n",
                "   - **MessagesPlaceholder** `variable_name='memory'`: Slot for conversation history\n",
                "   - **Human message** `('human', '{query}')`: Current user input\n",
                "3. **Memory list**: Stores conversation as message objects\n",
                "4. **Chat loop**:\n",
                "   - Get user input\n",
                "   - Invoke chain with `query` and `memory`\n",
                "   - Print response\n",
                "   - Append both messages to memory\n",
                "5. **Context awareness**: Model \"remembers\" previous exchanges via memory\n",
                "\n",
                "### How Memory Works:\n",
                "\n",
                "```python\n",
                "# Turn 1\n",
                "User: \"My name is Alice\"\n",
                "AI: \"Nice to meet you, Alice!\"\n",
                "memory = [HumanMessage(\"My name is Alice\"), AIMessage(\"Nice to meet you, Alice!\")]\n",
                "\n",
                "# Turn 2\n",
                "User: \"What's my name?\"\n",
                "# Model sees: system message + memory + current query\n",
                "AI: \"Your name is Alice\"\n",
                "```\n",
                "\n",
                "### MessagesPlaceholder Explained:\n",
                "\n",
                "- **Purpose**: Insert a list of messages dynamically\n",
                "- **Variable**: Must match the key in invoke dict\n",
                "- **Flexibility**: Can be empty list or contain many messages\n",
                "\n",
                "```python\n",
                "# Empty memory (first turn)\n",
                "chain.invoke({'query': 'Hello', 'memory': []})\n",
                "\n",
                "# With history (later turns)\n",
                "chain.invoke({'query': 'What did I ask?', 'memory': [HumanMessage(...), AIMessage(...)]})\n",
                "```\n",
                "\n",
                "### Alternative: Without Memory\n",
                "\n",
                "```python\n",
                "# Simple chat without memory\n",
                "prompt = ChatPromptTemplate([\n",
                "    ('system', 'You are a helpful assistant'),\n",
                "    ('human', '{question}')\n",
                "])\n",
                "\n",
                "chain = prompt | model\n",
                "response = chain.invoke({'question': 'What is AI?'})\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "message_types",
            "metadata": {},
            "source": [
                "## Message Types and Serialization\n",
                "\n",
                "### Message Classes\n",
                "\n",
                "LangChain provides specific classes for different message roles:\n",
                "\n",
                "```python\n",
                "from langchain_core.messages import (\n",
                "    SystemMessage,    # System instructions\n",
                "    HumanMessage,     # User input\n",
                "    AIMessage,        # Model response\n",
                "    FunctionMessage   # Function call results (for agents)\n",
                ")\n",
                "```\n",
                "\n",
                "### Why Use Message Objects?\n",
                "\n",
                "1. **Metadata Preservation**: Store additional info (timestamps, IDs)\n",
                "2. **Type Safety**: Clear distinction between roles\n",
                "3. **Compatibility**: Works with all chat models\n",
                "4. **Serialization**: Easy to save/load conversations\n",
                "\n",
                "### Creating Messages:\n",
                "\n",
                "```python\n",
                "# System message\n",
                "sys_msg = SystemMessage(content=\"You are a helpful assistant\")\n",
                "\n",
                "# Human message\n",
                "human_msg = HumanMessage(content=\"What is Python?\")\n",
                "\n",
                "# AI message\n",
                "ai_msg = AIMessage(content=\"Python is a programming language\")\n",
                "\n",
                "# With metadata\n",
                "msg = HumanMessage(\n",
                "    content=\"Hello\",\n",
                "    additional_kwargs={\"user_id\": \"123\", \"timestamp\": \"2024-02-04\"}\n",
                ")\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "message_serialization",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Message Serialization Helper\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "\n",
                "def messages_to_text(messages):\n",
                "    \"\"\"Convert message objects to readable text format.\"\"\"\n",
                "    lines = []\n",
                "    for msg in messages:\n",
                "        if isinstance(msg, HumanMessage):\n",
                "            lines.append(f\"Human: {msg.content}\")\n",
                "        elif isinstance(msg, AIMessage):\n",
                "            lines.append(f\"AI: {msg.content}\")\n",
                "    return \"\\n\".join(lines)\n",
                "\n",
                "# Example usage\n",
                "conversation = [\n",
                "    HumanMessage(content=\"What is LangChain?\"),\n",
                "    AIMessage(content=\"LangChain is a framework for building LLM applications.\"),\n",
                "    HumanMessage(content=\"What are its main components?\"),\n",
                "    AIMessage(content=\"Models, Prompts, Chains, Memory, and Agents.\")\n",
                "]\n",
                "\n",
                "print(messages_to_text(conversation))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "message_serialization_explanation",
            "metadata": {},
            "source": [
                "### Code Explanation:\n",
                "\n",
                "1. **Purpose**: Convert message objects to plain text\n",
                "2. **Use cases**:\n",
                "   - Logging conversations\n",
                "   - Saving to text files\n",
                "   - Displaying chat history\n",
                "   - Debugging\n",
                "3. **Type checking**: `isinstance()` identifies message type\n",
                "4. **Formatting**: Creates \"Role: Content\" format\n",
                "\n",
                "### Saving/Loading Conversations:\n",
                "\n",
                "```python\n",
                "import json\n",
                "from langchain_core.messages import messages_from_dict, messages_to_dict\n",
                "\n",
                "# Save to JSON\n",
                "messages_dict = messages_to_dict(conversation)\n",
                "with open('chat_history.json', 'w') as f:\n",
                "    json.dump(messages_dict, f)\n",
                "\n",
                "# Load from JSON\n",
                "with open('chat_history.json', 'r') as f:\n",
                "    loaded_dict = json.load(f)\n",
                "conversation = messages_from_dict(loaded_dict)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompt_engineering",
            "metadata": {},
            "source": [
                "## Prompt Engineering Best Practices\n",
                "\n",
                "### 1. Be Specific and Clear\n",
                "\n",
                "❌ **Bad**: \"Tell me about AI\"\n",
                "\n",
                "✅ **Good**: \"Explain artificial intelligence in 3 bullet points for a beginner\"\n",
                "\n",
                "### 2. Provide Context\n",
                "\n",
                "```python\n",
                "prompt = PromptTemplate.from_template(\n",
                "    \"\"\"\n",
                "    Context: You are a Python programming tutor.\n",
                "    Student Level: {level}\n",
                "    Question: {question}\n",
                "    \n",
                "    Provide a clear explanation with a code example.\n",
                "    \"\"\"\n",
                ")\n",
                "```\n",
                "\n",
                "### 3. Specify Output Format\n",
                "\n",
                "```python\n",
                "prompt = PromptTemplate.from_template(\n",
                "    \"\"\"\n",
                "    Analyze this text: {text}\n",
                "    \n",
                "    Provide your analysis in this format:\n",
                "    - Main Topic:\n",
                "    - Key Points:\n",
                "    - Sentiment:\n",
                "    \"\"\"\n",
                ")\n",
                "```\n",
                "\n",
                "### 4. Use Examples (Few-Shot Prompting)\n",
                "\n",
                "```python\n",
                "prompt = PromptTemplate.from_template(\n",
                "    \"\"\"\n",
                "    Classify the sentiment of the following text.\n",
                "    \n",
                "    Examples:\n",
                "    Text: \"I love this product!\"\n",
                "    Sentiment: Positive\n",
                "    \n",
                "    Text: \"This is terrible.\"\n",
                "    Sentiment: Negative\n",
                "    \n",
                "    Text: \"{text}\"\n",
                "    Sentiment:\n",
                "    \"\"\"\n",
                ")\n",
                "```\n",
                "\n",
                "### 5. Control Output Length\n",
                "\n",
                "```python\n",
                "prompt = PromptTemplate.from_template(\n",
                "    \"Summarize this in exactly 2 sentences: {text}\"\n",
                ")\n",
                "```\n",
                "\n",
                "### 6. System Message Best Practices\n",
                "\n",
                "```python\n",
                "# Good system messages\n",
                "system_messages = [\n",
                "    \"You are a helpful, concise assistant.\",\n",
                "    \"You are an expert Python programmer. Provide code examples.\",\n",
                "    \"You are a customer support agent. Be polite and professional.\",\n",
                "    \"You answer in bullet points. Be brief and direct.\"\n",
                "]\n",
                "```\n",
                "\n",
                "### 7. Temperature Settings\n",
                "\n",
                "| Task | Temperature | Reason |\n",
                "|------|-------------|--------|\n",
                "| Factual Q&A | 0.0-0.3 | Deterministic, accurate |\n",
                "| General Chat | 0.5-0.7 | Balanced |\n",
                "| Creative Writing | 0.8-1.0 | Diverse, creative |\n",
                "| Code Generation | 0.0-0.2 | Precise, correct |\n",
                "\n",
                "### 8. Prompt Testing\n",
                "\n",
                "```python\n",
                "# Test with multiple inputs\n",
                "test_cases = [\n",
                "    {\"topic\": \"AI\"},\n",
                "    {\"topic\": \"Python\"},\n",
                "    {\"topic\": \"LangChain\"}\n",
                "]\n",
                "\n",
                "for test in test_cases:\n",
                "    result = chain.invoke(test)\n",
                "    print(f\"Input: {test}\")\n",
                "    print(f\"Output: {result.content}\\n\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "advanced_techniques",
            "metadata": {},
            "source": [
                "## Advanced Prompting Techniques\n",
                "\n",
                "### 1. Chain-of-Thought Prompting\n",
                "\n",
                "Encourage step-by-step reasoning:\n",
                "\n",
                "```python\n",
                "prompt = PromptTemplate.from_template(\n",
                "    \"\"\"\n",
                "    Solve this problem step by step:\n",
                "    {problem}\n",
                "    \n",
                "    Let's think through this:\n",
                "    Step 1:\n",
                "    \"\"\"\n",
                ")\n",
                "```\n",
                "\n",
                "### 2. Role Prompting\n",
                "\n",
                "Assign a specific role:\n",
                "\n",
                "```python\n",
                "prompt = ChatPromptTemplate([\n",
                "    ('system', 'You are a senior software architect with 20 years of experience.'),\n",
                "    ('human', 'How should I design {feature}?')\n",
                "])\n",
                "```\n",
                "\n",
                "### 3. Constraint-Based Prompting\n",
                "\n",
                "Set clear boundaries:\n",
                "\n",
                "```python\n",
                "prompt = PromptTemplate.from_template(\n",
                "    \"\"\"\n",
                "    Write a summary of {text}\n",
                "    \n",
                "    Constraints:\n",
                "    - Maximum 50 words\n",
                "    - Use simple language\n",
                "    - Include only main points\n",
                "    \"\"\"\n",
                ")\n",
                "```\n",
                "\n",
                "### 4. Multi-Step Prompting\n",
                "\n",
                "Break complex tasks into steps:\n",
                "\n",
                "```python\n",
                "# Step 1: Generate ideas\n",
                "ideas_prompt = PromptTemplate.from_template(\n",
                "    \"Generate 5 ideas for {topic}\"\n",
                ")\n",
                "\n",
                "# Step 2: Evaluate ideas\n",
                "eval_prompt = PromptTemplate.from_template(\n",
                "    \"Evaluate these ideas and pick the best one: {ideas}\"\n",
                ")\n",
                "\n",
                "# Chain them\n",
                "chain = ideas_prompt | model | eval_prompt | model\n",
                "```\n",
                "\n",
                "### 5. Conditional Prompting\n",
                "\n",
                "Adapt based on input:\n",
                "\n",
                "```python\n",
                "def get_prompt(user_level):\n",
                "    if user_level == \"beginner\":\n",
                "        return PromptTemplate.from_template(\n",
                "            \"Explain {concept} in simple terms with examples\"\n",
                "        )\n",
                "    else:\n",
                "        return PromptTemplate.from_template(\n",
                "            \"Provide a technical explanation of {concept}\"\n",
                "        )\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Prompt Types Covered\n",
                "\n",
                "1. **PromptTemplate**: Simple string templates with variables\n",
                "2. **ChatPromptTemplate**: Role-based messages for conversations\n",
                "3. **MessagesPlaceholder**: Dynamic message insertion for memory\n",
                "4. **Message Types**: HumanMessage, AIMessage, SystemMessage\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **Templates > Hardcoding**: Always use templates for reusability\n",
                "2. **Chat Models Need ChatPromptTemplate**: Use role-based messages\n",
                "3. **System Messages Matter**: Set behavior and constraints\n",
                "4. **Memory = MessagesPlaceholder**: For conversation history\n",
                "5. **LCEL Integration**: Prompts chain seamlessly with models\n",
                "6. **Prompt Engineering**: Better prompts = better outputs\n",
                "\n",
                "### Quick Decision Guide\n",
                "\n",
                "```\n",
                "Simple completion? → PromptTemplate\n",
                "Chatbot/conversation? → ChatPromptTemplate\n",
                "Need memory? → ChatPromptTemplate + MessagesPlaceholder\n",
                "Complex task? → Multi-step prompting\n",
                "Need consistency? → System message with constraints\n",
                "```\n",
                "\n",
                "### Prompt Engineering Checklist\n",
                "\n",
                "✅ Clear instructions\n",
                "✅ Relevant context\n",
                "✅ Specified output format\n",
                "✅ Examples (if needed)\n",
                "✅ Appropriate temperature\n",
                "✅ Tested with multiple inputs\n",
                "\n",
                "### Next Steps in LangChain\n",
                "\n",
                "After mastering prompts:\n",
                "1. **Output Parsers**: Structure model outputs (JSON, Pydantic)\n",
                "2. **Chains**: Combine prompts, models, and parsers\n",
                "3. **Memory**: Advanced conversation management\n",
                "4. **Agents**: Build autonomous systems\n",
                "5. **RAG**: Retrieval-augmented generation\n",
                "\n",
                "### Additional Resources\n",
                "\n",
                "- [LangChain Prompts Documentation](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
                "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
                "- [OpenAI Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
