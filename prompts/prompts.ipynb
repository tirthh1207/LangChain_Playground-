{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2d827fea",
            "metadata": {},
            "source": [
                "### Prompts \n",
                "- Structured way to create prompts Dynamically by inserting into a predefined template. Instead of hardcoding prompts.\n",
                "- Reusable, Flexable and easy to manage, especialy when working with Dynamic user input or automation workflows.\n",
                "\n",
                "\n",
                "- Input to the Model: A prompt is technically the text input that you feed into an LLM to get a specific output. It is the \"command\" or \"query\" that triggers the model's generation.\n",
                "- Instruction & Context: Deep down, a prompt is more than just a question. It usually consists of a few key parts:\n",
                "    - Instruction: What you want the model to do (e.g., \"Summarize this,\" \"Translate this\").\n",
                "    - Context: Background information that helps the model answer    correctly.\n",
                "    - Input Data: The specific information you want the model to process.\n",
                "    - Output Indicator: How you want the answer formatted (e.g., \"Answer in a list,\" \"Return JSON\").\n",
                "- A \"Program\" for the LLM: In frameworks like LangChain, prompts are treated like code. Instead of hardcoding a string like \"Tell me a joke about cats\", we create Prompt Templates (e.g., \"Tell me a joke about {animal}\"). This allows us to reuse the same structure for different inputs dynamically.\n",
                "- Steering Mechanism: Prompts are the primary way to \"steer\" or control the probability distribution of the model's output. A better prompt leads to a more accurate and relevant response (this is the essence of Prompt Engineering)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53fd011a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prompt Template\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "prompt = PromptTemplate(\n",
                "    template=\"Generate few points on {topic}.\",\n",
                "    input_variables=['topic'])\n",
                "\n",
                "chain = prompt | model\n",
                "result = chain.invoke({'topic':'Ai'})\n",
                "print(result.content)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-1",
            "metadata": {},
            "source": [
                "### Theory: Basic Prompt Templates and LCEL\n",
                "\n",
                "1. **HuggingFaceEndpoint**: This class serves as a connector to models hosted on the Hugging Face Hub (via their Inference API). By specifying `repo_id`, `task`, and `temperature`, we configure which remote model to use and how it should behave (e.g., creativity vs. determinism).\n",
                "\n",
                "2. **ChatHuggingFace**: While `HuggingFaceEndpoint` provides a raw text completion interface, `ChatHuggingFace` wraps this to provide a chat-oriented interface (handling messages like System, User, AI) which is more compatible with modern instruction-tuned models.\n",
                "\n",
                "3. **PromptTemplate**: This is a core LangChain concept. Instead of hardcoding prompt strings (e.g., \"Tell me about AI\"), we create a template with placeholders (e.g., `{topic}`). This allows for:\n",
                "   - **Reusability**: The same structure can be used for different inputs.\n",
                "   - **Validation**: LangChain checks if input variables match the template.\n",
                "\n",
                "4. **LCEL (LangChain Expression Language)**:\n",
                "   - The syntax `chain = prompt | model` uses the Unix pipe operator `|` to chain components together.\n",
                "   - Data flows from left to right: The input dictionary `{'topic': 'Ai'}` goes into `prompt`, which formats the string, and the output string goes into `model`, which generates the response."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "8d2ad402",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-->AI: Hello!\n"
                    ]
                }
            ],
            "source": [
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "prompt = ChatPromptTemplate([\n",
                "    ('system', '''You are a Helpful AI assistent. your role is to give a short and straight-to-point, direct, concise responces.\n",
                "     Format: give your answer and then stop...'''),\n",
                "    MessagesPlaceholder(variable_name='memory'),\n",
                "    ('human','{query}')])\n",
                "\n",
                "memory=[]\n",
                "\n",
                "while True:\n",
                "    query = input('-->You: ')\n",
                "    if query.lower() =='exit':        \n",
                "        break\n",
                "    chain = prompt | model\n",
                "    responce = chain.invoke({'query': query,'memory':memory})\n",
                "    print('-->AI:',responce.content)\n",
                "    memory.append(HumanMessage(content=query))\n",
                "    memory.append(AIMessage(content=responce.content))\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bc89ea79",
            "metadata": {},
            "source": [
                "### Theory: Chat Templates and Memory Management\n",
                "\n",
                "1. **ChatPromptTemplate**: Unlike a simple string template, this models a conversation history. It accepts a list of role-based messages:\n",
                "   - `('system', ...)`: Sets the behavior or persona of the AI (e.g., \"You are a Helpful AI assistant\").\n",
                "   - `MessagesPlaceholder`: A placeholder for inserting a list of messages dynamically (used here for `memory`).\n",
                "   - `('human', '{query}')`: The user's current input.\n",
                "\n",
                "2. **Message Roles**:\n",
                "   - **HumanMessage**: Represents input from the user.\n",
                "   - **AIMessage**: Represents the response from the model.\n",
                "   - **SystemMessage**: (implicit in the tuple) High-level instructions that persist throughout the interaction.\n",
                "\n",
                "3. **Memory Loop**:\n",
                "   - The `while True` loop creates a continuous chat session.\n",
                "   - `memory.append(...)`: We manually store the conversation history.\n",
                "   - When `chain.invoke` is called, the current `query` AND the entire `memory` list are passed to the prompt template. This allows the model to \"remember\" previous context (like your name or previous questions) to generate a context-aware response."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "25a2c3ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "\n",
                "def messages_to_text(messages):\n",
                "    lines = []\n",
                "    for msg in messages:\n",
                "        if isinstance(msg, HumanMessage):\n",
                "            lines.append(f\"Human: {msg.content}\")\n",
                "        elif isinstance(msg, AIMessage):\n",
                "            lines.append(f\"AI: {msg.content}\")\n",
                "    return \"\\n\".join(lines)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-2",
            "metadata": {},
            "source": [
                "### Theory: Message Serialization\n",
                "\n",
                "1. **Message Objects**: In LangChain, chat history is stored as objects (`HumanMessage`, `AIMessage`, etc.) rather than simple strings. This preserves metadata and roles.\n",
                "\n",
                "2. **Serialization**: The function `messages_to_text` converts these objects back into a plain string format. This is often necessary for:\n",
                "   - **Logging/Debugging**: Printing the conversation history in a readable format.\n",
                "   - **Storage**: Saving history to a text file or database that doesn't support complex objects.\n",
                "   - **Model Compatibility**: Some older or raw completion models expect a single string blob rather than a structured list of messages."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
