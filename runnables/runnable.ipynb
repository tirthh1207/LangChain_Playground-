{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c42e3b6",
   "metadata": {},
   "source": [
    "# Sequential Chains\n",
    "\n",
    "In LangChain, a **RunnableSequence** allows you to link multiple steps together in a specific order, creating a \"chain\". The output of one step becomes the input for the next step.\n",
    "\n",
    "This is similar to a factory assembly line where raw material (input) passes through stations (prompt, model, parser) to become a finished product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential runnable\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Give some intresting facts about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(prompt1, model, parser)\n",
    "result = chain.invoke({'topic': 'AI'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d5093",
   "metadata": {},
   "source": [
    "# Parallel Execution\n",
    "\n",
    "**RunnableParallel** lets you run multiple independent tasks simultaneously based on the same input.\n",
    "\n",
    "For example, from a single topic, you can generate a summary, key notes, and facts all at once. The result is a dictionary containing the outputs from all parallel branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Runnable\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Artical research\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate 250 words summary on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate Bullet-point-notes on {topic}. cover only important points and make it concise.',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Generate key-facts on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt4 = PromptTemplate(\n",
    "    template='tell some possible applications on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt5 = PromptTemplate(\n",
    "    template='Generate a Detailed Report from the content: \\nSummary: {summary} \\nPoint-Notes: {notes} \\nKey-Facts: {facts} \\nApplication: {application}',\n",
    "    input_variables=['summary', 'notes', 'facts', 'application']\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'summary': RunnableSequence(prompt1, model, parser),\n",
    "    'notes': RunnableSequence(prompt2, model, parser),\n",
    "    'facts': RunnableSequence(prompt3, model, parser),\n",
    "    'application': RunnableSequence(prompt4, model, parser)\n",
    "})\n",
    "\n",
    "chain = RunnableSequence(parallel_chain, prompt5, model, parser)\n",
    "print(chain.invoke({'topic':'Agentic AI'}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d3c77",
   "metadata": {},
   "source": [
    "# Custom Data Flow and Logic\n",
    "\n",
    "As chains grow complex, you need precise control over data.\n",
    "\n",
    "*   **RunnablePassthrough**: Allows you to pass input data to later steps or add new keys to the existing data flow without overwriting everything.\n",
    "*   **RunnableLambda**: Wraps standard Python functions so they can be used as steps in a chain (e.g., for counting words or formatting text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ee01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runnable Passthrough and Lambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Explain {topic} in siple words.',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Give some key-points about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Generate some relivent question-answers about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "chain = RunnableSequence(prompt1, model, parser)\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"points\": RunnableSequence(prompt2, model, parser),\n",
    "    'quiz': RunnableSequence(prompt3, model, parser ),\n",
    "    'words': RunnableLambda(lambda x : len(x.split()))\n",
    "})\n",
    "pipeline = RunnableSequence(chain, parallel_chain)\n",
    "result = pipeline.invoke({'topic': 'AI'})\n",
    "print(result.keys())\n",
    "\n",
    "print()\n",
    "print(chain.invoke({'topic': 'ai'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7ecb5",
   "metadata": {},
   "source": [
    "# Conditional Logic (Branching)\n",
    "\n",
    "**RunnableBranch** allows your chain to make decisions, similar to an \"if-then-else\" statement.\n",
    "\n",
    "It evaluates a condition (like \"is the text longer than 500 words?\") and chooses which path to execute. This enables dynamic workflows that adapt to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Explain {topic} in siple words.',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize this text in unter 500 words.\\n Text: {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "chain1 = prompt1 | model | parser \n",
    "chain2 = RunnableBranch(\n",
    "    (lambda x : len(x.split()) > 500, chain1 | prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "chain3 = chain1 | chain2\n",
    "result = chain3.invoke({'topic': 'AI'})\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
