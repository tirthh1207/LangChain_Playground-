{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c42e3b6",
   "metadata": {},
   "source": [
    "# Runnables in LangChain\n",
    "\n",
    "## What are Runnables?\n",
    "\n",
    "**Runnables** are LangChain's fundamental building blocks for creating complex, composable workflows. They provide a unified interface for executing chains, allowing you to combine, branch, and parallelize operations with simple syntax.\n",
    "\n",
    "### Why Use Runnables?\n",
    "\n",
    "1. **Composability**: Chain operations together with `|` pipe operator\n",
    "2. **Flexibility**: Mix sequential, parallel, and conditional flows\n",
    "3. **Standardization**: Consistent interface across all components\n",
    "4. **Control**: Fine-grained control over data flow and logic\n",
    "5. **Reusability**: Build modular components that work together\n",
    "6. **Scalability**: Easy to extend with custom logic\n",
    "\n",
    "### Core Runnable Types\n",
    "\n",
    "| Type | Purpose | Use Case |\n",
    "|------|---------|----------|\n",
    "| **RunnableSequence** | Execute steps in order | Linear workflows |\n",
    "| **RunnableParallel** | Run independent tasks simultaneously | Multi-aspect analysis |\n",
    "| **RunnableBranch** | Choose paths based on conditions | Dynamic routing |\n",
    "| **RunnablePassthrough** | Pass data through without modification | Data flow control |\n",
    "| **RunnableLambda** | Wrap Python functions as runnables | Custom logic |\n",
    "\n",
    "### The Runnable Workflow\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "[Process] (Sequential/Parallel/Conditional)\n",
    "  ↓\n",
    "Intermediate Output\n",
    "  ↓\n",
    "[Process]\n",
    "  ↓\n",
    "Final Output\n",
    "```\n",
    "\n",
    "### Key Principle\n",
    "\n",
    "**Runnables enable powerful composition through simple syntax**. The `|` operator lets you build complex workflows that remain readable and maintainable.\n",
    "\n",
    "## Sequential Runnables\n",
    "\n",
    "### Definition\n",
    "\n",
    "A **RunnableSequence** executes multiple steps in a **strict linear order**. Each step completes before the next begins, with output automatically feeding into the next step's input.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Ordered Execution**: Steps run one after another\n",
    "- **Auto Data Passing**: Output → Input (no manual passing)\n",
    "- **LCEL Compatible**: Works with pipe operator `|`\n",
    "- **Chaining**: Easily combine prompts, models, parsers\n",
    "- **Linear Flow**: Perfect for sequential workflows\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Step-by-step content generation\n",
    "- Progressive transformations\n",
    "- Report generation workflows\n",
    "- Multi-stage processing pipelines\n",
    "- Data refinement (raw → structured)\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Use RunnableSequence when:**\n",
    "- Steps depend on previous output\n",
    "- Order matters\n",
    "- Simple linear flow\n",
    "- Transforming data step-by-step\n",
    "\n",
    "❌ **Don't use when:**\n",
    "- Steps are independent (use Parallel)\n",
    "- Need conditional branching (use Branch)\n",
    "- Complex custom logic needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential runnable\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Give some intresting facts about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(prompt1, model, parser)\n",
    "result = chain.invoke({'topic': 'AI'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d5093",
   "metadata": {},
   "source": [
    "## Parallel Runnables\n",
    "\n",
    "### Definition\n",
    "\n",
    "A **RunnableParallel** executes multiple **independent chains simultaneously** based on the same input. All branches run at once (if system allows) and results are combined into a dictionary.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Simultaneous Execution**: All branches run in parallel\n",
    "- **Independent Processing**: Each branch doesn't depend on others\n",
    "- **Dictionary Output**: Results keyed by branch names\n",
    "- **Efficient**: Faster than sequential for independent tasks\n",
    "- **Composable**: Can combine with sequential and conditional\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Input: {topic: 'AI'}\n",
    "    ↓\n",
    "┌───┴──────────────────┐\n",
    "│  RunnableParallel    │\n",
    "├───┬──────┬──────┬────┤\n",
    "│   │      │      │    │\n",
    "v   v      v      v    v\n",
    "[Branch 1] [Branch 2] [Branch 3]\n",
    "    ↓          ↓           ↓\n",
    "Result 1   Result 2    Result 3\n",
    "    └─────────┬─────────┘\n",
    "         ↓\n",
    "    {'branch1': ..., 'branch2': ..., 'branch3': ...}\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Generate multiple perspectives simultaneously\n",
    "- Create comprehensive analysis (summary + notes + facts)\n",
    "- Multi-model comparisons\n",
    "- Parallel content creation\n",
    "- Extract different information from same input\n",
    "- Content pipeline (different formats)\n",
    "\n",
    "### Advantages\n",
    "\n",
    "| Aspect | Sequential | Parallel |\n",
    "|--------|-----------|----------|\n",
    "| **Speed** | Slower | Faster |\n",
    "| **Independence** | Dependent steps | Independent steps |\n",
    "| **Complexity** | Simple | Medium |\n",
    "| **Output** | Single value | Dictionary |\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Use RunnableParallel when:**\n",
    "- Branches are independent\n",
    "- Need multiple outputs from one input\n",
    "- Branches run on different resources\n",
    "- Speed improvement needed\n",
    "\n",
    "❌ **Don't use when:**\n",
    "- Branches depend on each other\n",
    "- Single output needed\n",
    "- Sequential processing required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Runnable\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Artical research\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate 250 words summary on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate Bullet-point-notes on {topic}. cover only important points and make it concise.',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Generate key-facts on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt4 = PromptTemplate(\n",
    "    template='tell some possible applications on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt5 = PromptTemplate(\n",
    "    template='Generate a Detailed Report from the content: \\nSummary: {summary} \\nPoint-Notes: {notes} \\nKey-Facts: {facts} \\nApplication: {application}',\n",
    "    input_variables=['summary', 'notes', 'facts', 'application']\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'summary': RunnableSequence(prompt1, model, parser),\n",
    "    'notes': RunnableSequence(prompt2, model, parser),\n",
    "    'facts': RunnableSequence(prompt3, model, parser),\n",
    "    'application': RunnableSequence(prompt4, model, parser)\n",
    "})\n",
    "\n",
    "chain = RunnableSequence(parallel_chain, prompt5, model, parser)\n",
    "print(chain.invoke({'topic':'Agentic AI'}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d3c77",
   "metadata": {},
   "source": [
    "## Custom Data Flow and Logic\n",
    "\n",
    "### RunnablePassthrough\n",
    "\n",
    "**Definition**: Passes data through to later steps without modification, or adds new keys while preserving existing data.\n",
    "\n",
    "**Key Features**:\n",
    "- **Data Preservation**: Keeps original data intact\n",
    "- **No Modification**: Doesn't transform input\n",
    "- **Adding Keys**: Can add new data to existing\n",
    "- **Branching**: Enables parallel access to original input\n",
    "- **Flexibility**: Works with other runnables\n",
    "\n",
    "**Use Cases**:\n",
    "- Keep original input for later steps\n",
    "- Branch to use same input multiple times\n",
    "- Add metadata or computed fields\n",
    "- Enable parallel branches from same data\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Keep original while creating branches\n",
    "RunnableParallel({\n",
    "    'original': RunnablePassthrough(),\n",
    "    'processed': chain1,\n",
    "    'analyzed': chain2\n",
    "})\n",
    "# All get access to original input\n",
    "```\n",
    "\n",
    "### RunnableLambda\n",
    "\n",
    "**Definition**: Wraps regular Python functions so they work as runnables in chains, enabling custom logic.\n",
    "\n",
    "**Key Features**:\n",
    "- **Function Wrapping**: Converts Python functions to runnables\n",
    "- **Custom Logic**: Use any Python code in chains\n",
    "- **Data Transformation**: Filter, format, compute, etc.\n",
    "- **Simple Interface**: `RunnableLambda(lambda x: function(x))`\n",
    "- **Composable**: Works with pipes and other runnables\n",
    "\n",
    "**Use Cases**:\n",
    "- Count words or characters\n",
    "- Format/filter data\n",
    "- Perform calculations\n",
    "- Custom validation\n",
    "- Data transformation\n",
    "- Adding computed fields\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Count words\n",
    "word_counter = RunnableLambda(lambda x: len(x.split()))\n",
    "\n",
    "# Extract specific field\n",
    "extractor = RunnableLambda(lambda x: x['data']['value'])\n",
    "\n",
    "# Custom transformation\n",
    "formatter = RunnableLambda(lambda x: x.upper().strip())\n",
    "```\n",
    "\n",
    "### Comparison: Passthrough vs Lambda\n",
    "\n",
    "| Feature | Passthrough | Lambda |\n",
    "|---------|-------------|--------|\n",
    "| **Modifies Data** | No | Yes |\n",
    "| **Purpose** | Pass through | Custom logic |\n",
    "| **Use** | Data preservation | Transformation |\n",
    "| **Complexity** | None | Custom |\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Use Passthrough when:**\n",
    "- Need original input in branches\n",
    "- Avoiding data loss\n",
    "- Parallel access to input\n",
    "\n",
    "✅ **Use Lambda when:**\n",
    "- Custom data transformation needed\n",
    "- Computation required\n",
    "- Validation or filtering needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ee01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runnable Passthrough and Lambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Explain {topic} in siple words.',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Give some key-points about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Generate some relivent question-answers about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "chain = RunnableSequence(prompt1, model, parser)\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"points\": RunnableSequence(prompt2, model, parser),\n",
    "    'quiz': RunnableSequence(prompt3, model, parser ),\n",
    "    'words': RunnableLambda(lambda x : len(x.split()))\n",
    "})\n",
    "pipeline = RunnableSequence(chain, parallel_chain)\n",
    "result = pipeline.invoke({'topic': 'AI'})\n",
    "print(result.keys())\n",
    "\n",
    "print()\n",
    "print(chain.invoke({'topic': 'ai'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7ecb5",
   "metadata": {},
   "source": [
    "## Conditional Logic (RunnableBranch)\n",
    "\n",
    "### Definition\n",
    "\n",
    "A **RunnableBranch** enables **conditional execution** - choosing different processing paths based on input data. It implements \"if-then-else\" logic for dynamic workflows.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Decision Making**: Evaluate conditions on data\n",
    "- **Multiple Paths**: Different runnables for different conditions\n",
    "- **Dynamic Routing**: Route inputs to appropriate handlers\n",
    "- **Fallback**: Default path if no conditions match\n",
    "- **Composable**: Works with other runnables\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "[Evaluate Condition 1] → True → Execute Path A → Output A\n",
    "  ↓ False\n",
    "[Evaluate Condition 2] → True → Execute Path B → Output B\n",
    "  ↓ False\n",
    "[Evaluate Condition 3] → True → Execute Path C → Output C\n",
    "  ↓ False\n",
    "[Execute Default Path] → Output Default\n",
    "```\n",
    "\n",
    "### Syntax\n",
    "\n",
    "```python\n",
    "RunnableBranch(\n",
    "    (lambda x: condition1(x), runnable_if_true),\n",
    "    (lambda x: condition2(x), runnable_if_true),\n",
    "    default_runnable  # fallback if no conditions match\n",
    ")\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Content length → Different processors (summarize long, keep short)\n",
    "- User role → Different information levels\n",
    "- Input type → Different parsers\n",
    "- Sentiment → Different response templates\n",
    "- Language → Different language processors\n",
    "- Query complexity → Different search strategies\n",
    "- Data quality → Different validation chains\n",
    "\n",
    "### Condition Examples\n",
    "\n",
    "```python\n",
    "# Length-based routing\n",
    "lambda x: len(x.split()) > 500\n",
    "\n",
    "# Attribute-based routing\n",
    "lambda x: x.get('type') == 'email'\n",
    "\n",
    "# Boolean flag routing\n",
    "lambda x: x.get('is_premium', False)\n",
    "\n",
    "# Range-based routing\n",
    "lambda x: 18 <= x.get('age', 0) < 65\n",
    "\n",
    "# Complex condition\n",
    "lambda x: x.get('category') == 'urgent' and len(x.get('text', '')) > 1000\n",
    "```\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Use RunnableBranch when:**\n",
    "- Need conditional execution\n",
    "- Different paths for different inputs\n",
    "- Dynamic workflow routing\n",
    "- Multiple handlers for different cases\n",
    "\n",
    "❌ **Don't use when:**\n",
    "- Only one path needed\n",
    "- All branches always execute (use Parallel)\n",
    "- No conditions to evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3)\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Explain {topic} in siple words.',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize this text in unter 500 words.\\n Text: {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "chain1 = prompt1 | model | parser \n",
    "chain2 = RunnableBranch(\n",
    "    (lambda x : len(x.split()) > 500, chain1 | prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "chain3 = chain1 | chain2\n",
    "result = chain3.invoke({'topic': 'AI'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293c036",
   "metadata": {},
   "source": [
    "## Best Practices for Runnables\n",
    "\n",
    "### 1. Choose the Right Runnable Type\n",
    "\n",
    "**Decision Tree:**\n",
    "\n",
    "```\n",
    "Need multiple branches?\n",
    "├─ Branches independent? \n",
    "│  ├─ YES → RunnableParallel\n",
    "│  └─ NO → Sequential or Branch\n",
    "├─ Need conditional routing?\n",
    "│  ├─ YES → RunnableBranch\n",
    "│  └─ NO → RunnableSequence\n",
    "├─ Need to preserve input?\n",
    "│  ├─ YES → RunnablePassthrough\n",
    "│  └─ NO → Use pipe |\n",
    "└─ Need custom logic?\n",
    "   ├─ YES → RunnableLambda\n",
    "   └─ NO → Use component\n",
    "```\n",
    "\n",
    "### 2. Build Complex Workflows Systematically\n",
    "\n",
    "```python\n",
    "# ✅ GOOD: Build and test step-by-step\n",
    "step1 = prompt1 | model | parser\n",
    "step2 = prompt2 | model | parser\n",
    "\n",
    "# Test step1\n",
    "result1 = step1.invoke({'input': 'test'})\n",
    "assert result1  # Verify before combining\n",
    "\n",
    "# Test step2\n",
    "result2 = step2.invoke({'input': 'test'})\n",
    "assert result2\n",
    "\n",
    "# Now combine\n",
    "sequential = step1 | step2  # Safe - both tested\n",
    "\n",
    "# ❌ BAD: Build everything at once without testing\n",
    "chain = (\n",
    "    prompt1 | model | parser | \n",
    "    prompt2 | model | parser |\n",
    "    prompt3 | model | parser\n",
    ")  # Hard to debug if it fails\n",
    "```\n",
    "\n",
    "### 3. Handle Parallel Outputs Correctly\n",
    "\n",
    "```python\n",
    "# ✅ GOOD: Use dictionary keys to access parallel outputs\n",
    "parallel = RunnableParallel({\n",
    "    'summary': chain1,\n",
    "    'facts': chain2\n",
    "})\n",
    "\n",
    "next_chain = RunnableLambda(\n",
    "    lambda x: f\"Summary: {x['summary']}\\nFacts: {x['facts']}\"\n",
    ")\n",
    "\n",
    "combined = parallel | next_chain\n",
    "\n",
    "# ❌ BAD: Forget keys in next step\n",
    "next_chain = lambda x: x + \" more text\"  # x is a dict, not string!\n",
    "```\n",
    "\n",
    "### 4. Optimize Parallel Execution\n",
    "\n",
    "```python\n",
    "# ✅ GOOD: Independent branches in parallel\n",
    "parallel = RunnableParallel({\n",
    "    'summary': summarize_chain,    # Use different models\n",
    "    'entities': entity_chain,      # Independent analysis\n",
    "    'sentiment': sentiment_chain   # No dependencies\n",
    "})\n",
    "\n",
    "# ❌ BAD: Dependencies between parallel branches\n",
    "parallel = RunnableParallel({\n",
    "    'step1': chain1,\n",
    "    'step2': RunnableLambda(lambda x: process(x['step1']))  # Depends on step1!\n",
    "})\n",
    "# Should be sequential, not parallel\n",
    "```\n",
    "\n",
    "### 5. Error Handling and Validation\n",
    "\n",
    "```python\n",
    "from langchain_core.exceptions import LangChainException\n",
    "\n",
    "def safe_invoke(runnable, inputs):\n",
    "    try:\n",
    "        return runnable.invoke(inputs)\n",
    "    except LangChainException as e:\n",
    "        print(f\"Runnable error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test before production\n",
    "result = safe_invoke(chain, {'input': 'test'})\n",
    "```\n",
    "\n",
    "### 6. Performance Considerations\n",
    "\n",
    "```python\n",
    "# ✅ GOOD: Use parallel for truly independent tasks\n",
    "RunnableParallel({\n",
    "    'analysis1': expensive_chain1,  # Different models\n",
    "    'analysis2': expensive_chain2   # Run simultaneously\n",
    "})  # Much faster!\n",
    "\n",
    "# ✅ GOOD: Cache intermediate results\n",
    "def cached_chain():\n",
    "    intermediate = step1 | step2\n",
    "    # Reuse intermediate multiple times\n",
    "    return RunnableParallel({\n",
    "        'branch1': intermediate | step3a,\n",
    "        'branch2': intermediate | step3b\n",
    "    })\n",
    "\n",
    "# ❌ BAD: Sequential when parallel possible\n",
    "step1 | step2a | step2b | step3  # Slow - sequential\n",
    "# Better as: step1 | (step2a | step2b in parallel) | step3\n",
    "```\n",
    "\n",
    "### 7. Composition Patterns\n",
    "\n",
    "```python\n",
    "# Pattern 1: Simple sequence\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Pattern 2: Sequential with multiple steps\n",
    "chain = prompt1 | model | parser | prompt2 | model | parser\n",
    "\n",
    "# Pattern 3: Parallel analysis\n",
    "chain = RunnableParallel({\n",
    "    'summary': prompt1 | model | parser,\n",
    "    'facts': prompt2 | model | parser,\n",
    "    'keywords': prompt3 | model | parser\n",
    "}) | merge_prompt | model | parser\n",
    "\n",
    "# Pattern 4: Conditional routing\n",
    "classifier = prompt_classify | model | parser_classify\n",
    "routes = RunnableBranch(\n",
    "    (lambda x: x.get('type') == 'long', long_handler),\n",
    "    (lambda x: x.get('type') == 'short', short_handler),\n",
    "    default_handler\n",
    ")\n",
    "chain = classifier | routes\n",
    "\n",
    "# Pattern 5: Complex workflow\n",
    "analysis = RunnableParallel({...})\n",
    "conditional = RunnableBranch(...)\n",
    "chain = input_processor | analysis | conditional | formatter\n",
    "```\n",
    "\n",
    "## Summary: Runnables\n",
    "\n",
    "### Runnable Types Quick Reference\n",
    "\n",
    "| Type | Use | Output | Speed |\n",
    "|------|-----|--------|-------|\n",
    "| **Sequence** | Linear flow | Single | Slowest |\n",
    "| **Parallel** | Independent tasks | Dictionary | Fastest |\n",
    "| **Branch** | Conditional routing | Route-specific | Medium |\n",
    "| **Passthrough** | Data preservation | Unchanged | Instant |\n",
    "| **Lambda** | Custom logic | Transformed | Depends |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Composition is Powerful**: `|` operator makes complex flows readable\n",
    "2. **Sequential**: Output → Input (automatic data passing)\n",
    "3. **Parallel**: Speed gains for independent operations\n",
    "4. **Conditional**: Route based on data properties\n",
    "5. **Custom Logic**: RunnableLambda for transformations\n",
    "6. **Test Incrementally**: Build and test components separately\n",
    "7. **Dictionary Keys**: Access parallel outputs by key names\n",
    "8. **Error Handling**: Implement try-except for production\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Need | Runnable Type |\n",
    "|------|---------------|\n",
    "| Linear workflow | Sequence (`\\|`) |\n",
    "| Multiple branches | Parallel |\n",
    "| If-then-else logic | Branch |\n",
    "| Keep original data | Passthrough |\n",
    "| Custom function | Lambda |\n",
    "| Combine all above | Nested composition |\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "✅ **DO:**\n",
    "- Use Parallel for independent tasks\n",
    "- Test components individually\n",
    "- Cache expensive chains\n",
    "- Handle errors gracefully\n",
    "- Monitor chain execution\n",
    "\n",
    "❌ **DON'T:**\n",
    "- Force sequential when parallel possible\n",
    "- Skip validation and testing\n",
    "- Create circular dependencies\n",
    "- Ignore error handling\n",
    "- Build massive chains without tests\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Pattern: Research article generation\n",
    "parallel = RunnableParallel({\n",
    "    'summary': summary_chain,\n",
    "    'outline': outline_chain,\n",
    "    'details': details_chain\n",
    "})\n",
    "\n",
    "merge = RunnableLambda(\n",
    "    lambda x: f\"Summary: {x['summary']}\\n\\n\" +\n",
    "              f\"Outline: {x['outline']}\\n\\n\" +\n",
    "              f\"Details: {x['details']}\"\n",
    ")\n",
    "\n",
    "format_chain = merge | format_prompt | model | parser\n",
    "\n",
    "result = parallel | format_chain\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "After mastering runnables:\n",
    "1. **Memory**: Add conversation history\n",
    "2. **Agents**: Build autonomous decision systems\n",
    "3. **RAG**: Implement retrieval-augmented generation\n",
    "4. **Custom Runnables**: Create specialized components\n",
    "5. **Production**: Deploy with monitoring and logging\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [LangChain Runnable Docs](https://python.langchain.com/docs/expression_language/runnables/)\n",
    "- [LCEL Specification](https://python.langchain.com/docs/expression_language/)\n",
    "- [Runnable API Reference](https://api.python.langchain.com/en/latest/runnables/)\n",
    "- [LangChain Examples](https://python.langchain.com/docs/expression_language/cookbook/)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
