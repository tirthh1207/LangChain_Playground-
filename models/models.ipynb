{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_section",
   "metadata": {},
   "source": [
    "# LangChain Models\n",
    "\n",
    "## What are Models in LangChain?\n",
    "\n",
    "**Models** are the core building blocks of LangChain applications. They provide a **standardized interface** to interact with various Large Language Model (LLM) providers, allowing you to switch between different models and providers without changing your application code.\n",
    "\n",
    "### Why Use LangChain Models?\n",
    "\n",
    "1. **Provider Abstraction**: Write code once, use with any LLM provider (OpenAI, HuggingFace, Groq, Ollama, etc.)\n",
    "2. **Consistent Interface**: All models use the same `.invoke()` method regardless of provider\n",
    "3. **Easy Switching**: Change from one model to another with minimal code changes\n",
    "4. **Composability**: Models integrate seamlessly with prompts, chains, and other LangChain components\n",
    "5. **Type Safety**: Chat models provide structured message handling\n",
    "\n",
    "### Models in the LangChain Ecosystem\n",
    "\n",
    "```\n",
    "Input → Prompt Template → Model → Output Parser → Result\n",
    "```\n",
    "\n",
    "Models sit at the heart of every LangChain application, processing prompts and generating responses.\n",
    "\n",
    "### Key Principle\n",
    "\n",
    "**All LangChain models implement a common interface**, making them interchangeable and composable with other LangChain components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llms_vs_chat",
   "metadata": {},
   "source": [
    "## LLMs vs Chat Models\n",
    "\n",
    "### Definition\n",
    "\n",
    "LangChain provides two main types of model interfaces:\n",
    "\n",
    "#### 1. LLMs (Language Models)\n",
    "- **Input**: Plain text string\n",
    "- **Output**: Plain text string\n",
    "- **Use Case**: Simple text completion, generation tasks\n",
    "- **Example**: \"Complete this sentence: The capital of France is\" → \"Paris\"\n",
    "\n",
    "#### 2. Chat Models\n",
    "- **Input**: List of messages with roles (System, Human, AI)\n",
    "- **Output**: AI message with content\n",
    "- **Use Case**: Conversational AI, chatbots, multi-turn dialogues\n",
    "- **Example**: \n",
    "  ```python\n",
    "  [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\")\n",
    "  ]\n",
    "  → AIMessage(content=\"The capital of France is Paris.\")\n",
    "  ```\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | LLMs | Chat Models |\n",
    "|---------|------|-------------|\n",
    "| **Input Format** | String | List of Messages |\n",
    "| **Output Format** | String | AIMessage |\n",
    "| **Conversation History** | Manual | Built-in |\n",
    "| **Role Support** | No | Yes (System, Human, AI) |\n",
    "| **Best For** | Simple completion | Conversations, agents |\n",
    "| **Example Class** | `HuggingFaceEndpoint` | `ChatHuggingFace`, `ChatGroq` |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use LLMs when:**\n",
    "- Simple text completion tasks\n",
    "- Single-turn generation\n",
    "- No conversation context needed\n",
    "\n",
    "**Use Chat Models when:**\n",
    "- Building chatbots or assistants\n",
    "- Multi-turn conversations\n",
    "- Need system instructions\n",
    "- Working with agents\n",
    "- Most modern LLM applications (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_providers",
   "metadata": {},
   "source": [
    "## Model Providers Overview\n",
    "\n",
    "LangChain supports integration with numerous LLM providers:\n",
    "\n",
    "### API-Based Providers\n",
    "- **OpenAI**: GPT-4, GPT-3.5 (requires API key, paid)\n",
    "- **Anthropic**: Claude models (requires API key, paid)\n",
    "- **Google**: Gemini, PaLM (requires API key)\n",
    "- **Groq**: Ultra-fast inference (requires API key, free tier available)\n",
    "- **HuggingFace**: Access to thousands of models (requires API token, free tier)\n",
    "\n",
    "### Local Providers\n",
    "- **Ollama**: Run models locally (free, requires local installation)\n",
    "- **HuggingFace Pipeline**: Download and run models locally (free, requires storage)\n",
    "- **LlamaCpp**: Optimized local inference (free)\n",
    "\n",
    "### Comparison: API vs Local\n",
    "\n",
    "| Aspect | API-Based | Local |\n",
    "|--------|-----------|-------|\n",
    "| **Setup** | Easy (just API key) | Complex (install, download) |\n",
    "| **Cost** | Pay per token | Free (after hardware) |\n",
    "| **Speed** | Fast (dedicated servers) | Depends on hardware |\n",
    "| **Privacy** | Data sent to provider | Complete privacy |\n",
    "| **Model Selection** | Provider's models | Any open-source model |\n",
    "| **Hardware Needs** | None | GPU recommended |\n",
    "| **Internet** | Required | Not required |\n",
    "\n",
    "In this notebook, we'll cover:\n",
    "1. **HuggingFaceEndpoint** (API-based)\n",
    "2. **ChatGroq** (API-based, very fast)\n",
    "3. **ChatOllama** (Local)\n",
    "4. **HuggingFacePipeline** (Local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "huggingface_intro",
   "metadata": {},
   "source": [
    "## HuggingFaceEndpoint\n",
    "\n",
    "### Definition\n",
    "\n",
    "`HuggingFaceEndpoint` provides access to models hosted on the **Hugging Face Hub** via their Inference API. This allows you to use thousands of open-source models without downloading them locally.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Access to 1000s of models**: Any model on Hugging Face Hub\n",
    "- **No local storage**: Models run on Hugging Face servers\n",
    "- **Free tier available**: Limited requests per month\n",
    "- **API token required**: Get from huggingface.co/settings/tokens\n",
    "- **Flexible**: Supports various tasks (text-generation, summarization, etc.)\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Trying different models without downloading\n",
    "- Production apps with moderate traffic\n",
    "- Prototyping and experimentation\n",
    "- Using specialized models (code, multilingual, etc.)\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Rate limits**: Free tier has request limits\n",
    "- **Internet required**: Cannot work offline\n",
    "- **Latency**: Slower than local models\n",
    "- **Privacy**: Data sent to Hugging Face servers\n",
    "\n",
    "### ChatHuggingFace Wrapper\n",
    "\n",
    "`ChatHuggingFace` wraps a `HuggingFaceEndpoint` to make it behave like a chat model, enabling:\n",
    "- Message-based input (System, Human, AI)\n",
    "- Conversation history management\n",
    "- Integration with LangChain chat components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "huggingface_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFaceEndpoint and ChatHuggingFace Example\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (HF_TOKEN should be in .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# Create HuggingFace endpoint for Llama model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.2-1B-Instruct\",  # Model ID from Hugging Face Hub\n",
    "    task=\"text-generation\",                      # Task type\n",
    "    temperature=0.5                               # Creativity level (0-1)\n",
    ")\n",
    "\n",
    "# Wrap it as a chat model\n",
    "model1 = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Test the model\n",
    "response = model1.invoke(\"What is LangChain?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "huggingface_explanation",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Import required classes**: `HuggingFaceEndpoint` for API access, `ChatHuggingFace` for chat wrapper\n",
    "2. **Load environment variables**: `load_dotenv()` loads `HF_TOKEN` from `.env` file\n",
    "   - Get your token from: https://huggingface.co/settings/tokens\n",
    "   - Add to `.env`: `HF_TOKEN=your_token_here`\n",
    "3. **Create endpoint**:\n",
    "   - `repo_id`: Model identifier from Hugging Face Hub (format: `username/model-name`)\n",
    "   - `task`: Type of task (\"text-generation\" for LLMs)\n",
    "   - `temperature`: Controls randomness (0 = deterministic, 1 = creative)\n",
    "4. **Wrap as chat model**: `ChatHuggingFace(llm=llm)` converts the LLM to a chat model\n",
    "5. **Invoke**: `model1.invoke(\"question\")` sends request and returns `AIMessage`\n",
    "6. **Access content**: `response.content` contains the generated text\n",
    "\n",
    "### Common Parameters:\n",
    "\n",
    "```python\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"model-name\",           # Required: Model ID\n",
    "    task=\"text-generation\",         # Task type\n",
    "    temperature=0.7,                # Randomness (0-1)\n",
    "    max_new_tokens=512,             # Max tokens to generate\n",
    "    top_p=0.95,                     # Nucleus sampling\n",
    "    repetition_penalty=1.15,        # Prevent repetition\n",
    "    huggingfacehub_api_token=\"...\" # Or use env variable\n",
    ")\n",
    "```\n",
    "\n",
    "### Popular Models to Try:\n",
    "\n",
    "- `meta-llama/Llama-3.2-1B-Instruct` - Fast, small Llama model\n",
    "- `mistralai/Mistral-7B-Instruct-v0.2` - High quality, efficient\n",
    "- `google/flan-t5-large` - Good for summarization\n",
    "- `bigcode/starcoder` - Code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatgroq_intro",
   "metadata": {},
   "source": [
    "## ChatGroq\n",
    "\n",
    "### Definition\n",
    "\n",
    "`ChatGroq` provides access to **Groq's ultra-fast inference API**. Groq uses custom hardware (LPU - Language Processing Unit) to achieve extremely fast response times.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Blazing Fast**: 10-100x faster than typical inference\n",
    "- **Low Latency**: Responses in milliseconds\n",
    "- **Free Tier**: Generous free tier available\n",
    "- **Popular Models**: Llama, Mixtral, Gemma\n",
    "- **Chat-Native**: Built for conversational AI\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Real-time chatbots\n",
    "- Interactive applications\n",
    "- High-throughput applications\n",
    "- Streaming responses\n",
    "- Production apps requiring speed\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Limited model selection**: Only models optimized for Groq hardware\n",
    "- **API key required**: Get from console.groq.com\n",
    "- **Rate limits**: Free tier has limits\n",
    "- **Internet required**: Cloud-based only\n",
    "\n",
    "### Why Choose Groq?\n",
    "\n",
    "**Speed is critical** → Groq is the fastest option\n",
    "- User-facing chatbots (no waiting)\n",
    "- Real-time applications\n",
    "- High request volume\n",
    "- Streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chatgroq_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGroq Example\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (GROQ_API_KEY should be in .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# Create Groq chat model\n",
    "model2 = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Model name\n",
    "    temperature=0.5                 # Creativity level\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "response = model2.invoke(\"Explain LangChain in one sentence\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatgroq_explanation",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Import ChatGroq**: From `langchain_groq` package\n",
    "2. **Load environment**: `load_dotenv()` loads `GROQ_API_KEY` from `.env`\n",
    "   - Get API key from: https://console.groq.com/keys\n",
    "   - Add to `.env`: `GROQ_API_KEY=your_key_here`\n",
    "3. **Create model**:\n",
    "   - `model`: Model identifier (see available models below)\n",
    "   - `temperature`: Controls randomness (0 = focused, 1 = creative)\n",
    "4. **Invoke**: Send message and get response\n",
    "5. **Access content**: `response.content` contains the answer\n",
    "\n",
    "### Available Groq Models:\n",
    "\n",
    "```python\n",
    "# Fast and efficient\n",
    "model=\"llama-3.1-8b-instant\"      # Llama 3.1 8B (recommended)\n",
    "model=\"llama-3.1-70b-versatile\"   # Llama 3.1 70B (more capable)\n",
    "model=\"mixtral-8x7b-32768\"        # Mixtral (long context)\n",
    "model=\"gemma-7b-it\"               # Google Gemma\n",
    "```\n",
    "\n",
    "### Common Parameters:\n",
    "\n",
    "```python\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,              # Randomness (0-1)\n",
    "    max_tokens=1024,              # Max response length\n",
    "    top_p=0.9,                    # Nucleus sampling\n",
    "    stream=False,                 # Enable streaming\n",
    "    api_key=\"...\"                 # Or use env variable\n",
    ")\n",
    "```\n",
    "\n",
    "### Performance Comparison:\n",
    "\n",
    "| Provider | Typical Latency | Speed Rating |\n",
    "|----------|----------------|---------------|\n",
    "| Groq | 50-200ms | ⚡⚡⚡⚡⚡ |\n",
    "| OpenAI | 1-3s | ⚡⚡⚡ |\n",
    "| HuggingFace API | 2-5s | ⚡⚡ |\n",
    "| Local (GPU) | 500ms-2s | ⚡⚡⚡ |\n",
    "| Local (CPU) | 5-30s | ⚡ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatollama_intro",
   "metadata": {},
   "source": [
    "## ChatOllama\n",
    "\n",
    "### Definition\n",
    "\n",
    "`ChatOllama` provides an interface to **Ollama**, a tool for running large language models locally on your machine. It's the easiest way to run models locally.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **100% Local**: Models run entirely on your machine\n",
    "- **Complete Privacy**: No data sent to external servers\n",
    "- **Free**: No API costs\n",
    "- **Offline**: Works without internet\n",
    "- **Easy Setup**: Simple installation and model management\n",
    "- **Many Models**: Llama, Mistral, Phi, Gemma, and more\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Privacy-sensitive applications\n",
    "- Offline environments\n",
    "- Development without API costs\n",
    "- Learning and experimentation\n",
    "- Applications with high request volume\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Hardware requirements**: Needs good CPU/GPU\n",
    "- **Storage**: Models can be 1-40GB each\n",
    "- **Setup required**: Must install Ollama first\n",
    "- **Slower than Groq**: But faster than CPU-only inference\n",
    "- **Model size constraints**: Larger models need more RAM\n",
    "\n",
    "### Installation\n",
    "\n",
    "1. **Install Ollama**: Download from https://ollama.ai\n",
    "2. **Pull a model**: \n",
    "   ```bash\n",
    "   ollama pull phi3\n",
    "   ollama pull llama3.2\n",
    "   ollama pull mistral\n",
    "   ```\n",
    "3. **Verify**: `ollama list` to see installed models\n",
    "\n",
    "### When to Use Ollama?\n",
    "\n",
    "**Choose Ollama when:**\n",
    "- Privacy is critical\n",
    "- No internet connection\n",
    "- Want to avoid API costs\n",
    "- Have decent hardware (8GB+ RAM)\n",
    "- Learning/experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chatollama_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama Example\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# PREREQUISITE: Run this in terminal first:\n",
    "# ollama pull phi3\n",
    "\n",
    "# Create Ollama chat model\n",
    "model3 = ChatOllama(\n",
    "    model='phi3',        # Model name (must be pulled first)\n",
    "    temperature=0.5      # Creativity level\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "response = model3.invoke(\"What are the benefits of local LLMs?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatollama_explanation",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Import ChatOllama**: From `langchain_ollama` package\n",
    "2. **Prerequisites**: \n",
    "   - Ollama must be installed\n",
    "   - Model must be pulled: `ollama pull phi3`\n",
    "   - Ollama service must be running (starts automatically on install)\n",
    "3. **Create model**:\n",
    "   - `model`: Name of pulled model\n",
    "   - `temperature`: Controls randomness\n",
    "4. **Invoke**: Works exactly like other chat models\n",
    "5. **Local execution**: Model runs on your machine, no API calls\n",
    "\n",
    "### Popular Ollama Models:\n",
    "\n",
    "| Model | Size | Best For | RAM Needed |\n",
    "|-------|------|----------|------------|\n",
    "| `phi3` | 2.3GB | Fast, efficient | 4GB |\n",
    "| `llama3.2` | 2GB | General purpose | 4GB |\n",
    "| `mistral` | 4.1GB | High quality | 8GB |\n",
    "| `llama3.1:8b` | 4.7GB | Balanced | 8GB |\n",
    "| `codellama` | 3.8GB | Code generation | 8GB |\n",
    "| `llama3.1:70b` | 40GB | Best quality | 64GB |\n",
    "\n",
    "### Common Parameters:\n",
    "\n",
    "```python\n",
    "model = ChatOllama(\n",
    "    model=\"phi3\",\n",
    "    temperature=0.7,\n",
    "    num_predict=512,        # Max tokens to generate\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    repeat_penalty=1.1,\n",
    "    base_url=\"http://localhost:11434\"  # Ollama server URL\n",
    ")\n",
    "```\n",
    "\n",
    "### Ollama Commands:\n",
    "\n",
    "```bash\n",
    "# List installed models\n",
    "ollama list\n",
    "\n",
    "# Pull a model\n",
    "ollama pull llama3.2\n",
    "\n",
    "# Remove a model\n",
    "ollama rm phi3\n",
    "\n",
    "# Run model in terminal (for testing)\n",
    "ollama run phi3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_intro",
   "metadata": {},
   "source": [
    "## HuggingFacePipeline (Fully Local)\n",
    "\n",
    "### Definition\n",
    "\n",
    "`HuggingFacePipeline` allows you to download and run models **entirely locally** using the `transformers` library. This gives you complete control and maximum privacy.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Fully Local**: Model weights downloaded to your machine\n",
    "- **Complete Privacy**: No external API calls\n",
    "- **Free**: No API costs\n",
    "- **Offline**: Works without internet (after download)\n",
    "- **Full Control**: Access to all model parameters\n",
    "- **Any Model**: Use any model from Hugging Face Hub\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Maximum privacy requirements\n",
    "- Air-gapped environments\n",
    "- Research and experimentation\n",
    "- Fine-tuning custom models\n",
    "- High-volume applications (no API limits)\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Hardware intensive**: Requires good GPU for reasonable speed\n",
    "- **Large storage**: Models are 1-40GB+\n",
    "- **Complex setup**: More configuration than Ollama\n",
    "- **Slow on CPU**: Can take 10-30 seconds per response\n",
    "- **Memory hungry**: Large models need 16GB+ RAM\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- **GPU recommended**: NVIDIA GPU with CUDA for good performance\n",
    "- **Storage**: 10-50GB for model weights\n",
    "- **RAM**: 8GB minimum, 16GB+ recommended\n",
    "- **Libraries**: `transformers`, `torch`, `accelerate`\n",
    "\n",
    "### When to Use HuggingFacePipeline?\n",
    "\n",
    "**Choose HuggingFacePipeline when:**\n",
    "- Need absolute privacy (medical, legal, sensitive data)\n",
    "- Offline environment required\n",
    "- Want to fine-tune models\n",
    "- Have powerful hardware (GPU)\n",
    "- Research or experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFacePipeline Example (Fully Local)\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "import os\n",
    "\n",
    "# Set cache directory (where models will be downloaded)\n",
    "os.environ['HF_HOME'] = 'D:/huggingface_cache'\n",
    "\n",
    "# Create local pipeline\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',  # Small model for demo\n",
    "    task='text-generation',                         # Task type\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,                            # Creativity\n",
    "        max_new_tokens=100                          # Max response length\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wrap as chat model\n",
    "model4 = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Test the model (runs locally!)\n",
    "result = model4.invoke(\"What is the capital of India?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_explanation",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Import classes**: `HuggingFacePipeline` for local inference, `ChatHuggingFace` for chat wrapper\n",
    "2. **Set cache directory**: `HF_HOME` determines where models are downloaded\n",
    "   - Default: `~/.cache/huggingface/`\n",
    "   - Change if you have limited space on C: drive\n",
    "3. **Create pipeline**:\n",
    "   - `model_id`: Model from Hugging Face Hub\n",
    "   - `task`: Type of task\n",
    "   - `pipeline_kwargs`: Parameters for generation\n",
    "4. **First run**: Model will be downloaded (can take several minutes)\n",
    "5. **Subsequent runs**: Uses cached model (fast startup)\n",
    "6. **Wrap as chat**: `ChatHuggingFace(llm=llm)` for chat interface\n",
    "7. **Invoke**: Runs entirely on your machine\n",
    "\n",
    "### Recommended Local Models:\n",
    "\n",
    "| Model | Size | Speed (CPU) | Speed (GPU) | Quality |\n",
    "|-------|------|-------------|-------------|----------|\n",
    "| `TinyLlama/TinyLlama-1.1B-Chat-v1.0` | 1.1GB | Slow | Fast | Basic |\n",
    "| `microsoft/phi-2` | 2.7GB | Very Slow | Fast | Good |\n",
    "| `mistralai/Mistral-7B-Instruct-v0.2` | 14GB | Unusable | Medium | Excellent |\n",
    "| `meta-llama/Llama-2-7b-chat-hf` | 13GB | Unusable | Medium | Excellent |\n",
    "\n",
    "### Advanced Configuration:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load model with specific device\n",
    "model_id = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",      # Automatically use GPU if available\n",
    "    torch_dtype=\"auto\",     # Optimize data type\n",
    "    trust_remote_code=True  # For some models\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Use with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "```\n",
    "\n",
    "### GPU vs CPU Performance:\n",
    "\n",
    "| Hardware | TinyLlama (1.1B) | Phi-2 (2.7B) | Mistral (7B) |\n",
    "|----------|------------------|--------------|---------------|\n",
    "| **CPU** | 5-10s | 15-30s | 60-120s |\n",
    "| **GPU (RTX 3060)** | 0.5-1s | 1-2s | 3-5s |\n",
    "| **GPU (RTX 4090)** | 0.2-0.5s | 0.5-1s | 1-2s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_parameters",
   "metadata": {},
   "source": [
    "## Understanding Model Parameters\n",
    "\n",
    "### Temperature\n",
    "\n",
    "**Controls randomness/creativity of responses**\n",
    "\n",
    "- **Range**: 0.0 to 1.0 (sometimes up to 2.0)\n",
    "- **Low (0.0-0.3)**: Deterministic, focused, factual\n",
    "- **Medium (0.4-0.7)**: Balanced, natural\n",
    "- **High (0.8-1.0+)**: Creative, varied, unpredictable\n",
    "\n",
    "**Use Cases:**\n",
    "```python\n",
    "temperature=0.0   # Factual Q&A, code generation, data extraction\n",
    "temperature=0.5   # General chatbot, balanced responses\n",
    "temperature=0.9   # Creative writing, brainstorming, storytelling\n",
    "```\n",
    "\n",
    "### Max Tokens / Max New Tokens\n",
    "\n",
    "**Limits the length of generated response**\n",
    "\n",
    "- **max_tokens**: Total tokens (prompt + response)\n",
    "- **max_new_tokens**: Only response tokens (recommended)\n",
    "- **Typical values**: 100-2000\n",
    "\n",
    "**Guidelines:**\n",
    "```python\n",
    "max_new_tokens=50     # Short answers, classifications\n",
    "max_new_tokens=256    # Paragraph responses\n",
    "max_new_tokens=1024   # Detailed explanations\n",
    "max_new_tokens=2048   # Long-form content\n",
    "```\n",
    "\n",
    "### Top P (Nucleus Sampling)\n",
    "\n",
    "**Alternative to temperature for controlling randomness**\n",
    "\n",
    "- **Range**: 0.0 to 1.0\n",
    "- **How it works**: Considers only top P% probability mass\n",
    "- **Typical value**: 0.9-0.95\n",
    "\n",
    "```python\n",
    "top_p=0.9   # Consider top 90% of probability mass\n",
    "top_p=0.95  # More diverse (default for many models)\n",
    "top_p=1.0   # Consider all tokens\n",
    "```\n",
    "\n",
    "### Top K\n",
    "\n",
    "**Limits vocabulary to top K most likely tokens**\n",
    "\n",
    "- **Range**: 1 to vocabulary size\n",
    "- **Typical value**: 40-50\n",
    "- **Lower = more focused, Higher = more diverse**\n",
    "\n",
    "```python\n",
    "top_k=10   # Very focused\n",
    "top_k=40   # Balanced (common default)\n",
    "top_k=100  # More diverse\n",
    "```\n",
    "\n",
    "### Repetition Penalty\n",
    "\n",
    "**Prevents model from repeating itself**\n",
    "\n",
    "- **Range**: 1.0 to 2.0\n",
    "- **1.0**: No penalty\n",
    "- **1.1-1.3**: Recommended range\n",
    "- **Higher**: Stronger penalty against repetition\n",
    "\n",
    "```python\n",
    "repetition_penalty=1.0   # No penalty\n",
    "repetition_penalty=1.15  # Mild penalty (recommended)\n",
    "repetition_penalty=1.5   # Strong penalty\n",
    "```\n",
    "\n",
    "### Parameter Combinations:\n",
    "\n",
    "```python\n",
    "# Factual, deterministic\n",
    "temperature=0.1, top_p=0.9, top_k=40\n",
    "\n",
    "# Balanced, natural\n",
    "temperature=0.7, top_p=0.95, top_k=50\n",
    "\n",
    "# Creative, diverse\n",
    "temperature=0.9, top_p=0.98, top_k=100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "## Best Practices for LangChain Models\n",
    "\n",
    "### 1. Model Selection\n",
    "\n",
    "| Requirement | Recommended Model |\n",
    "|-------------|-------------------|\n",
    "| **Speed is critical** | ChatGroq |\n",
    "| **Privacy required** | ChatOllama or HuggingFacePipeline |\n",
    "| **Cost-effective** | HuggingFaceEndpoint (free tier) |\n",
    "| **Best quality** | ChatGroq (llama-3.1-70b) or GPT-4 |\n",
    "| **Offline needed** | ChatOllama or HuggingFacePipeline |\n",
    "| **Experimentation** | HuggingFaceEndpoint |\n",
    "| **Production** | ChatGroq or OpenAI |\n",
    "\n",
    "### 2. API Key Management\n",
    "\n",
    "✅ **DO:**\n",
    "- Store API keys in `.env` file\n",
    "- Use `python-dotenv` to load keys\n",
    "- Add `.env` to `.gitignore`\n",
    "- Use environment variables in production\n",
    "- Rotate keys regularly\n",
    "\n",
    "❌ **DON'T:**\n",
    "- Hardcode API keys in code\n",
    "- Commit keys to version control\n",
    "- Share keys publicly\n",
    "- Use same key across projects\n",
    "\n",
    "**Example `.env` file:**\n",
    "```\n",
    "HF_TOKEN=hf_xxxxxxxxxxxxx\n",
    "GROQ_API_KEY=gsk_xxxxxxxxxxxxx\n",
    "OPENAI_API_KEY=sk-xxxxxxxxxxxxx\n",
    "```\n",
    "\n",
    "### 3. Error Handling\n",
    "\n",
    "```python\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "try:\n",
    "    response = model.invoke(\"Your question\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    # Fallback logic\n",
    "```\n",
    "\n",
    "### 4. Rate Limiting\n",
    "\n",
    "```python\n",
    "import time\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "questions = [\"Q1\", \"Q2\", \"Q3\"]\n",
    "\n",
    "for q in questions:\n",
    "    response = model.invoke(q)\n",
    "    print(response.content)\n",
    "    time.sleep(1)  # Avoid rate limits\n",
    "```\n",
    "\n",
    "### 5. Cost Optimization\n",
    "\n",
    "- **Use smaller models** when possible (phi3 vs llama-70b)\n",
    "- **Limit max_tokens** to reduce costs\n",
    "- **Cache responses** for repeated queries\n",
    "- **Use local models** for development\n",
    "- **Monitor usage** with provider dashboards\n",
    "\n",
    "### 6. Performance Optimization\n",
    "\n",
    "```python\n",
    "# For local models: use GPU\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# For API models: use async for parallel requests\n",
    "import asyncio\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "async def get_response(question):\n",
    "    model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "    return await model.ainvoke(question)\n",
    "\n",
    "# Process multiple questions in parallel\n",
    "questions = [\"Q1\", \"Q2\", \"Q3\"]\n",
    "responses = await asyncio.gather(*[get_response(q) for q in questions])\n",
    "```\n",
    "\n",
    "### 7. Testing Models\n",
    "\n",
    "```python\n",
    "# Test with simple question first\n",
    "test_question = \"What is 2+2?\"\n",
    "response = model.invoke(test_question)\n",
    "print(f\"Model works: {response.content}\")\n",
    "\n",
    "# Verify model parameters\n",
    "print(f\"Model: {model.model}\")\n",
    "print(f\"Temperature: {model.temperature}\")\n",
    "```\n",
    "\n",
    "### 8. Model Switching\n",
    "\n",
    "```python\n",
    "# Easy to switch between models\n",
    "def get_model(provider=\"groq\"):\n",
    "    if provider == \"groq\":\n",
    "        return ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "    elif provider == \"ollama\":\n",
    "        return ChatOllama(model=\"phi3\")\n",
    "    elif provider == \"hf\":\n",
    "        llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "        return ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Use any model with same interface\n",
    "model = get_model(\"groq\")\n",
    "response = model.invoke(\"Hello!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Models Covered\n",
    "\n",
    "1. **HuggingFaceEndpoint**: API access to Hugging Face models\n",
    "2. **ChatHuggingFace**: Chat wrapper for HuggingFace models\n",
    "3. **ChatGroq**: Ultra-fast inference with Groq's LPU\n",
    "4. **ChatOllama**: Easy local model execution\n",
    "5. **HuggingFacePipeline**: Fully local model inference\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Standardized Interface**: All models use `.invoke()` - easy to switch\n",
    "2. **Chat Models Recommended**: Better for most modern applications\n",
    "3. **Speed**: Groq > Local GPU > HuggingFace API > Local CPU\n",
    "4. **Privacy**: Local (Ollama/Pipeline) > API-based\n",
    "5. **Cost**: Local (free) > HuggingFace (free tier) > Groq (free tier) > OpenAI (paid)\n",
    "6. **Parameters Matter**: Temperature, max_tokens, top_p affect output quality\n",
    "\n",
    "### Quick Decision Guide\n",
    "\n",
    "```\n",
    "Need speed? → ChatGroq\n",
    "Need privacy? → ChatOllama or HuggingFacePipeline\n",
    "Need free? → HuggingFaceEndpoint or ChatOllama\n",
    "Need offline? → ChatOllama or HuggingFacePipeline\n",
    "Need best quality? → ChatGroq (70B) or GPT-4\n",
    "Just learning? → HuggingFaceEndpoint or ChatOllama\n",
    "```\n",
    "\n",
    "### Next Steps in LangChain\n",
    "\n",
    "After understanding models:\n",
    "1. **Prompts**: Learn to craft effective prompts with PromptTemplate\n",
    "2. **Output Parsers**: Structure model outputs (JSON, Pydantic)\n",
    "3. **Chains**: Combine models with prompts and parsers\n",
    "4. **Memory**: Add conversation history\n",
    "5. **Agents**: Build autonomous AI systems\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [LangChain Models Documentation](https://python.langchain.com/docs/modules/model_io/models/)\n",
    "- [Hugging Face Hub](https://huggingface.co/models)\n",
    "- [Groq Console](https://console.groq.com)\n",
    "- [Ollama Models](https://ollama.ai/library)\n",
    "- [Model Comparison](https://artificialanalysis.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
