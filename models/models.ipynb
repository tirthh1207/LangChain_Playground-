{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96fdbf1",
   "metadata": {},
   "source": [
    "## 1. Models\n",
    "In LangChain, \"Models\" are the standardized interfaces to interact with various LLM providers.\n",
    "\n",
    "*   **LLMs vs Chat Models**:\n",
    "    *   **LLMs**: Take a text string as input and return a text string.\n",
    "    *   **Chat Models**: Take a list of messages (User/System/AI) and return a message.\n",
    "*   **Integrations**: LangChain supports many providers like Hugging Face, OpenAI, Groq, and Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1c00d",
   "metadata": {},
   "source": [
    "### Model Initialization Examples\n",
    "The following code demonstrates how to initialize different types of models:\n",
    "\n",
    "*   **HuggingFaceEndpoint**: access models hosted on the Hugging Face Hub (requires API token).\n",
    "*   **ChatHuggingFace**: Wrapper to make the endpoint behave like a chat model.\n",
    "*   **ChatGroq**: Interface for Groq's fast inference API.\n",
    "*   **ChatOllama**: Interface for running local models via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "model1 = ChatHuggingFace(llm=llm)\n",
    "\n",
    "llm1 = HuggingFaceEndpoint(\n",
    "    repo_id=\"MiniMaxAI/MiniMax-M2.1\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm = llm1)\n",
    "\n",
    "# Another model\n",
    "model2 = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.5\n",
    ")\n",
    "# Ollama Local Model: first do ollama pull phi3 -> keep it in terminal\n",
    "model3 = ChatOllama(\n",
    "    model='phi3',\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b119f5d",
   "metadata": {},
   "source": [
    "### Running Models Locally (HuggingFace Pipeline)\n",
    "\n",
    "This demonstrates running a model entirely on your local machine using the `transformers` library pipeline.\n",
    "\n",
    "*   **HuggingFacePipeline**: Loads the model weights into local memory (RAM/VRAM).\n",
    "*   **Pros**: Privacy, no API costs, offline access.\n",
    "*   **Cons**: Requires good hardware (GPU recommended), initial download size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce11964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = 'D:/huggingface_cache'\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")\n",
    "model4 = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model4.invoke(\"What is the capital of India\")\n",
    "\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
