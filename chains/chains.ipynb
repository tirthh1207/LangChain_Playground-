{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "425a0e8a",
            "metadata": {},
            "source": [
                "### What are Chains?\n",
                "In LangChain, a **Chain** is a sequence of calls that are linked together. The most common type of chain combines a **Prompt**, a **Model** (LLM), and an **Output Parser**.\n",
                "- **Composability**: Chains allow you to build complex applications by gluing together smaller, independent components.\n",
                "- **LCEL (LangChain Expression Language)**: The modern way to create chains using the `|` pipe operator (e.g., `chain = prompt | model | parser`).\n",
                "\n",
                "### Simple Chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed5e3183",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports \n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Model\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "# Parser\n",
                "parser = StrOutputParser()\n",
                "\n",
                "# Prompt\n",
                "template = PromptTemplate(\n",
                "    template='Generate some intresting facts about {topic} in 5 lines.',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "# Chain\n",
                "chain = template | model | parser\n",
                "result = chain.invoke({'topic': 'Agentic AI'})\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-1",
            "metadata": {},
            "source": [
                "### Theory: The Basic Chain (LCEL)\n",
                "1. **The Pipe Operator (`|`)**:\n",
                "   - This operator takes the output from the left side and feeds it as input to the right side.\n",
                "   - `prompt | model | parser` means:\n",
                "     1. Dictionary input `{'topic': '...'}` -> **Prompt** -> String Prompt\n",
                "     2. String Prompt -> **Model** -> Message Output\n",
                "     3. Message Output -> **Parser** -> Final String\n",
                "\n",
                "2. **Invoke**:\n",
                "   - `chain.invoke(...)` triggers the execution of this pipeline."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d0e9fdd2",
            "metadata": {},
            "source": [
                "### Sequential Chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3a6ee0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports \n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Model\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "# Parser\n",
                "parser = StrOutputParser()\n",
                "\n",
                "# Prompt\n",
                "prompt1 = PromptTemplate(\n",
                "    template='Generate a Detailed concised report about {topic}.',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "prompt2 = PromptTemplate(\n",
                "    template='Summarise a report in concised short points covering only important things.\\n Report:{topic}',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "chain = prompt1 | model | parser | prompt2 | model | parser\n",
                "result = chain.invoke({'topic':'Agentic AI'})\n",
                "\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-2",
            "metadata": {},
            "source": [
                "### Theory: Sequential Chains\n",
                "1. **Flow of Data**:\n",
                "   - In a sequential chain, the **output** of the first chain becomes the **input** of the second chain.\n",
                "   - Here, we pipeline everything in one go: `prompt1 | model | parser | prompt2 | model | parser`.\n",
                "\n",
                "2. **Automatic variable mapping**:\n",
                "   - The output of the first parser is a string (the \"Detailed report\").\n",
                "   - `prompt2` expects a variable `{topic}`.\n",
                "   - LangChain automatically passes the output of the previous step into the input of the next. So the \"report\" string fills the `{topic}` slot in `prompt2`."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77548cad",
            "metadata": {},
            "source": [
                "### Parallel Chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9f594bfb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports \n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.runnables import RunnableParallel\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Model\n",
                "llm1 = HuggingFaceEndpoint(\n",
                "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model1 = ChatHuggingFace(llm=llm1)\n",
                "\n",
                "llm2 = HuggingFaceEndpoint(\n",
                "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model2 = ChatHuggingFace(llm=llm2)\n",
                "\n",
                "prompt1 = PromptTemplate(\n",
                "    template='Generate short and simple notes about {topic}',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "prompt2 = PromptTemplate(\n",
                "    template='Generate 5 short question answers from the following topic \\n {topic}',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "prompt3 = PromptTemplate(\n",
                "    template='Merge the provided notes and quiz into a single document \\n notes -> {notes} and quiz -> {quiz}',\n",
                "    input_variables=['notes', 'quiz']\n",
                ")\n",
                "\n",
                "parser = StrOutputParser()\n",
                "\n",
                "parallel_chain = RunnableParallel({\n",
                "    'notes': prompt1 | model1 | parser,\n",
                "    'quiz': prompt2 | model2 | parser\n",
                "})\n",
                "\n",
                "merge_chain = prompt3 | model1 | parser\n",
                "\n",
                "chain = parallel_chain | merge_chain\n",
                "\n",
                "result = chain.invoke({'topic': 'Agentic AI'})\n",
                "print(result)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-3",
            "metadata": {},
            "source": [
                "### Theory: Parallel Execution (`RunnableParallel`)\n",
                "1. **Efficiency**:\n",
                "   - Sometimes we need to do two independent things at once (e.g., writing notes AND writing a quiz).\n",
                "   - `RunnableParallel` runs these tasks **simultaneously** (in parallel), saving time compared to running them one after another.\n",
                "\n",
                "2. **Branching and Merging**:\n",
                "   - **Branch**: The chain splits into two paths (`notes` and `quiz`).\n",
                "   - **Merge**: The output of `parallel_chain` is a dictionary: `{'notes': '...', 'quiz': '...'}`.\n",
                "   - `prompt3` is designed to take these exact keys (`{notes}` and `{quiz}`) and combine them into a final document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0672cb37",
            "metadata": {},
            "outputs": [],
            "source": [
                "# imports\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
                "from langchain_core.runnables import RunnableParallel, RunnableBranch, RunnableLambda\n",
                "from pydantic import BaseModel, Field\n",
                "from typing import Literal\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "# Models\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"deepseek-ai/DeepSeek-V3.2\",\n",
                "    task=\"text-generation\")\n",
                "model = ChatHuggingFace(llm = llm)\n",
                "\n",
                "llm2 = HuggingFaceEndpoint(\n",
                "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.7)\n",
                "model2 = ChatHuggingFace(llm = llm2)\n",
                "\n",
                "#parsers\n",
                "parser = StrOutputParser()\n",
                "\n",
                "class Feedback(BaseModel):\n",
                "\n",
                "    sentiment : Literal['positive', 'negative'] = Field(description='Give the sentiment of the feedback')\n",
                "# since it is a llm we dont have control on its responce so we need to structure the output using Pydantic output parser to get controled and consistent output.\n",
                "parser2 = PydanticOutputParser(pydantic_object=Feedback)\n",
                "\n",
                "# review\n",
                "review = '''WI got a defective ASUS laptop — it would shut down during normal use, the screen was flickering, it would hang, and I couldn’t close apps from the taskbar. I complained to Amazon customer care within 3 days, while it was still under the replacement period. They told me to contact ASUS support.\n",
                "\n",
                "I spoke with ASUS customer care, and they sent a technician. He checked and confirmed the problem, then said Windows would need to be reinstalled. I told him that this is a new product, and it shouldn’t arrive defective — I wanted a replacement, not a repair. He wrote down the issue in a letter and said he’d check with the company and get back to me.\n",
                "\n",
                "Two days passed with no response. When I called again, they said the replacement request was cancelled. I contacted customer care again, and they sent another technician. He submitted a replacement request and took a copy of the invoice.\n",
                "\n",
                "Now, for a replacement, a DOA (Dead on Arrival) letter is required, which is made using the details from the invoice. But my invoice didn’t have the laptop’s serial number. Because of that, ASUS refused to issue the DOA letter, saying you needed to talk to Amazon.\n",
                "\n",
                "After talking to Amazon 5–7 times, they kept saying their policy doesn’t allow adding a serial number to the invoice. ASUS said they can’t make the DOA letter without it. Later, I gave them the warranty slip, and after 2–3 days they somehow managed to create the letter.\n",
                "\n",
                "But when I tried to submit the replacement request on Amazon, they said replacement isn’t available at all — it won’t happen. They told me to contact the manufacturer or seller. The seller’s phone didn’t even connect. ASUS then said it’s Amazon’s issue, not theirs.\n",
                "\n",
                "So now both of them are refusing to take responsibility, and the replacement just isn’t happening.\n",
                "\n",
                "I recieved the defective laptop on 27 September , and today is 17 october and still nothing has changed.'''\n",
                "\n",
                "# get detailed summary of review\n",
                "prompt = PromptTemplate(\n",
                "    template='''You are a customer experience analyst.\n",
                "\n",
                "Summarize the following customer review clearly and objectively.\n",
                "\n",
                "Guidelines:\n",
                "\n",
                "Identify whether the review is positive, negative, or mixed\n",
                "\n",
                "Highlight the main points the customer mentioned\n",
                "\n",
                "Do NOT add assumptions or extra information\n",
                "\n",
                "Keep it short and factual (2–4 bullet points or 2–3 lines)\n",
                "\n",
                "Customer Review:\n",
                "{feedback}''',\n",
                "    input_variables=['feedback'])\n",
                "\n",
                "# Get positive or negative\n",
                "prompt1 = PromptTemplate(\n",
                "    template=' Classify the sentiment of the following feedback text into possitive or negative:\\n {feedback}. \\n {format_instruction}',\n",
                "    input_variables=['feedback'],\n",
                "    partial_variables={'format_instruction': parser2.get_format_instructions()})\n",
                "\n",
                "# Responce for positive review\n",
                "prompt2 = PromptTemplate(\n",
                "    template='''You are a customer support manager for a professional company.\n",
                "\n",
                "Write a calm, polite, and empathetic response to the following negative customer review.\n",
                "\n",
                "Guidelines:\n",
                "\n",
                "Acknowledge the customer’s concern clearly\n",
                "\n",
                "Apologize where appropriate (without admitting legal fault)\n",
                "\n",
                "Offer a solution or next step\n",
                "\n",
                "Keep the tone respectful and professional\n",
                "\n",
                "Do not argue or sound defensive\n",
                "\n",
                "Keep it concise but helpful\n",
                "\n",
                "Customer Review:\n",
                "{feedback}\n",
                "''',\n",
                "    input_variables=['feedback'])\n",
                "\n",
                "# Responce for negative review\n",
                "prompt3 = PromptTemplate(\n",
                "    template='''You are a customer support manager for a professional company.\n",
                "\n",
                "Write a warm, appreciative, and professional response to the following positive customer review.\n",
                "\n",
                "Guidelines:\n",
                "\n",
                "Thank the customer genuinely\n",
                "\n",
                "Acknowledge what they liked specifically\n",
                "\n",
                "Reinforce the company’s commitment to quality/service\n",
                "\n",
                "Keep the tone friendly but professional\n",
                "\n",
                "Keep it concise\n",
                "\n",
                "Customer Review:\n",
                "{feedback}\n",
                "''',\n",
                "    input_variables=['feedback'])\n",
                "\n",
                "# summary of review\n",
                "sentiment = prompt | model | parser\n",
                "review_summary = sentiment.invoke({'feedback': review})\n",
                "\n",
                "# classify positive or negaive\n",
                "classifier_chain = prompt1 | model | parser2\n",
                "\n",
                "# conditional runnable for positive or negative responce\n",
                "branched_chain = RunnableBranch(\n",
                "    (lambda x: x.sentiment == 'positive', prompt3 | model2 | parser),\n",
                "    (lambda x: x.sentiment == 'negative', prompt2 | model2 | parser),\n",
                "    RunnableLambda(lambda x: ' we could not find sentiment'))\n",
                "\n",
                "# final chain for responce\n",
                "chain = classifier_chain | branched_chain\n",
                "result = chain.invoke({'feedback': review})\n",
                "\n",
                "# Output\n",
                "print('--> sentiment:\\n',review_summary)\n",
                "print()\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-4",
            "metadata": {},
            "source": [
                "### Theory: Conditional Logic (`RunnableBranch`)\n",
                "1. **Routing Logic**:\n",
                "   - Real-world apps need decision making. \"If user is happy, thank them. If unhappy, apologize.\"\n",
                "   - `RunnableBranch` allows us to define these conditions.\n",
                "   - Format: `(condition_function, chain_to_run_if_true)`.\n",
                "\n",
                "2. **Classification & control**:\n",
                "   - We utilize a **PydanticOutputParser** first to strictly classify the sentiment as either `'positive'` or `'negative'`.\n",
                "   - The branch then checks `x.sentiment == 'positive'` to decide which prompt to use next."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
