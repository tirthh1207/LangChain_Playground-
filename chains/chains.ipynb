{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "425a0e8a",
            "metadata": {},
            "source": [
                "# Chains in LangChain\n",
                "\n",
                "## What are Chains?\n",
                "\n",
                "**Chains** are sequences of calls that link multiple components together to create a complete workflow. A chain typically combines a **Prompt Template**, a **Model (LLM)**, and an **Output Parser** into a unified pipeline.\n",
                "\n",
                "### Why Use Chains?\n",
                "\n",
                "1. **Composability**: Build complex applications by connecting independent components\n",
                "2. **Modularity**: Each component (prompt, model, parser) can be tested separately\n",
                "3. **Reusability**: Create once, use multiple times with different inputs\n",
                "4. **LCEL Integration**: Use the modern pipe operator `|` for elegant, readable code\n",
                "5. **Flexibility**: Easy to modify, extend, or rearrange components\n",
                "6. **Error Handling**: Chain failures can be caught and handled systematically\n",
                "\n",
                "### Chains in the LangChain Ecosystem\n",
                "\n",
                "```\n",
                "Input → Prompt Template → Model → Output Parser → Output\n",
                "        ↑___________ Chain Pipeline ___________↑\n",
                "```\n",
                "\n",
                "Chains orchestrate the flow of data through your LangChain application, transforming raw inputs into processed outputs.\n",
                "\n",
                "### Key Principle\n",
                "\n",
                "**Chains are the \"glue\" that holds LangChain components together**. They enable LCEL (LangChain Expression Language) syntax: `prompt | model | parser`, making your code clean and composable.\n",
                "\n",
                "## Chain Types\n",
                "\n",
                "| Type | Purpose | Complexity |\n",
                "|------|---------|-----------|\n",
                "| **Simple Chain** | Single linear flow: prompt → model → parser | Beginner |\n",
                "| **Sequential Chain** | Multiple steps where output feeds to next input | Intermediate |\n",
                "| **Parallel Chain** | Multiple independent branches running simultaneously | Intermediate |\n",
                "| **Conditional Chain** | Branching logic based on conditions | Advanced |\n",
                "| **Custom Chain** | Complex data flow with custom logic (RunnablePassthrough, RunnableLambda) | Advanced |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed5e3183",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports \n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Model\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "# Parser\n",
                "parser = StrOutputParser()\n",
                "\n",
                "# Prompt\n",
                "template = PromptTemplate(\n",
                "    template='Generate some intresting facts about {topic} in 5 lines.',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "# Chain\n",
                "chain = template | model | parser\n",
                "result = chain.invoke({'topic': 'Agentic AI'})\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-1",
            "metadata": {},
            "source": [
                "## Simple Chain (LCEL)\n",
                "\n",
                "### Definition\n",
                "\n",
                "A **Simple Chain** is the most basic type of chain, combining exactly three components in a linear sequence: Prompt Template → Model → Output Parser.\n",
                "\n",
                "### Key Features\n",
                "\n",
                "- **Linear Flow**: Single path from input to output\n",
                "- **LCEL Syntax**: Uses the pipe operator `|` for clean, readable code\n",
                "- **Automatic Passing**: Output of each step automatically becomes input to the next\n",
                "- **Type Safe**: Each component handles type conversions\n",
                "- **Fast Execution**: Minimal overhead for straightforward workflows\n",
                "\n",
                "### Use Cases\n",
                "\n",
                "- Single-turn question answering\n",
                "- Text summarization\n",
                "- Content generation with specific formatting\n",
                "- Basic text transformation\n",
                "- Simple classification tasks\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "✅ **DO:**\n",
                "- Keep prompts clear and specific\n",
                "- Use appropriate output parsers (StrOutputParser for strings, JsonOutputParser for structured data)\n",
                "- Test each component independently before chaining\n",
                "- Add error handling for production use\n",
                "\n",
                "❌ **DON'T:**\n",
                "- Chain too many steps without testing intermediate outputs\n",
                "- Use complex prompts without clear instructions\n",
                "- Forget to specify output parser\n",
                "- Hardcode values that should be template variables\n",
                "\n",
                "### Basic Syntax\n",
                "\n",
                "```python\n",
                "chain = prompt_template | model | output_parser\n",
                "result = chain.invoke({'variable': 'value'})\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d0e9fdd2",
            "metadata": {},
            "source": [
                "### Sequential Chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3a6ee0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports \n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Model\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "# Parser\n",
                "parser = StrOutputParser()\n",
                "\n",
                "# Prompt\n",
                "prompt1 = PromptTemplate(\n",
                "    template='Generate a Detailed concised report about {topic}.',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "prompt2 = PromptTemplate(\n",
                "    template='Summarise a report in concised short points covering only important things.\\n Report:{topic}',\n",
                "    input_variables=['topic']\n",
                ")\n",
                "\n",
                "chain = prompt1 | model | parser | prompt2 | model | parser\n",
                "result = chain.invoke({'topic':'Agentic AI'})\n",
                "\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-2",
            "metadata": {},
            "source": [
                "### Code Explanation: Simple Chain\n",
                "\n",
                "1. **Import Components**: `PromptTemplate`, `HuggingFaceEndpoint`, `ChatHuggingFace`, `StrOutputParser`\n",
                "2. **Initialize Model**: Set up LLM with specific repo_id and temperature\n",
                "3. **Create Prompt Template**: Define template with variables to be filled\n",
                "4. **Create Parser**: Use `StrOutputParser` to extract text from model response\n",
                "5. **Build Chain**: Use pipe operator `|` to connect components\n",
                "6. **Invoke Chain**: Pass input dictionary to trigger execution\n",
                "7. **Get Result**: Access the final processed output\n",
                "\n",
                "### Key Points\n",
                "\n",
                "- **The Pipe Operator (`|`)**: Takes output from left side and feeds as input to right side\n",
                "  - `prompt | model | parser` executes: \n",
                "    - Input dict → Prompt (formats) → String\n",
                "    - String → Model (generates) → AIMessage\n",
                "    - AIMessage → Parser (extracts text) → Clean String\n",
                "- **LCEL Advantage**: No manual variable passing needed; automatic type conversion\n",
                "- **Scalability**: Easy to add more steps: `prompt | model | parser | next_prompt | model | parser`\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77548cad",
            "metadata": {},
            "source": [
                "## Sequential Chain\n",
                "\n",
                "### Definition\n",
                "\n",
                "A **Sequential Chain** links multiple chains together where the **output of one chain becomes the input of the next chain**. This enables multi-step workflows like generating content then summarizing it.\n",
                "\n",
                "### Key Features\n",
                "\n",
                "- **Multi-Step Processing**: Execute multiple transformations sequentially\n",
                "- **Data Flow Continuity**: Output automatically feeds into next step's input\n",
                "- **Progressive Refinement**: Each step can modify or enhance the data\n",
                "- **Automatic Variable Mapping**: No manual variable assignment needed\n",
                "- **LCEL Compatible**: Built with pipe operator syntax\n",
                "\n",
                "### Use Cases\n",
                "\n",
                "- Generate detailed content → Summarize it\n",
                "- Brainstorm ideas → Evaluate and rank them\n",
                "- Extract information → Format and structure it\n",
                "- Draft text → Polish and review → Finalize\n",
                "- Multi-stage data processing pipelines\n",
                "\n",
                "### Advantages Over Simple Chains\n",
                "\n",
                "| Aspect | Simple Chain | Sequential Chain |\n",
                "|--------|--------------|------------------|\n",
                "| **Steps** | 1 (prompt → model → parser) | Multiple (chain → chain → ...) |\n",
                "| **Output Reuse** | Not directly | Each step output becomes input |\n",
                "| **Use Case** | Single transformation | Multi-stage workflows |\n",
                "| **Complexity** | Low | Medium |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9f594bfb",
            "metadata": {},
            "outputs": [],
            "source": [
                "## Parallel Chain\n",
                "\n",
                "### Definition\n",
                "\n",
                "A **Parallel Chain** executes multiple independent chains simultaneously based on the **same input**. Each branch runs independently, and their results are combined into a single output dictionary.\n",
                "\n",
                "### Key Features\n",
                "\n",
                "- **Simultaneous Execution**: Multiple chains run at the same time\n",
                "- **Independent Branches**: Each chain processes the same input separately\n",
                "- **Merged Results**: Dictionary containing all parallel outputs\n",
                "- **Efficient Processing**: Faster than sequential chains for independent tasks\n",
                "- **RunnableParallel**: LangChain's component for parallel execution\n",
                "\n",
                "### Use Cases\n",
                "\n",
                "- Generate multiple perspectives on a topic\n",
                "- Create summary + key points + quiz from one input\n",
                "- Parallel sentiment analysis and entity extraction\n",
                "- Multi-model comparisons (run with different models)\n",
                "- Content generation with different tones/styles\n",
                "- Research paper: abstract + methodology + results from same topic\n",
                "\n",
                "### Advantages\n",
                "\n",
                "| Aspect | Sequential Chain | Parallel Chain |\n",
                "|--------|------------------|-----------------|\n",
                "| **Speed** | Slower (step by step) | Faster (simultaneous) |\n",
                "| **Dependencies** | Each step depends on previous | Independent steps |\n",
                "| **Use Case** | Pipeline processing | Multi-aspect analysis |\n",
                "| **Output** | Single value | Dictionary of results |\n",
                "\n",
                "### Parallel Chain Architecture\n",
                "\n",
                "```\n",
                "         ┌─→ Prompt1 → Model → Parser → Output1\n",
                "Input ───┼─→ Prompt2 → Model → Parser → Output2  (all run simultaneously)\n",
                "         └─→ Prompt3 → Model → Parser → Output3\n",
                "\n",
                "Result: {'output1': value, 'output2': value, 'output3': value}\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-3",
            "metadata": {},
            "source": [
                "### Code Explanation: Sequential Chain\n",
                "\n",
                "1. **First Chain**: `prompt1 | model | parser`\n",
                "   - Takes `{topic}` as input\n",
                "   - Generates detailed report about the topic\n",
                "   - Parses output as string\n",
                "   - Result: Full report text\n",
                "\n",
                "2. **Second Chain**: `prompt2 | model | parser`\n",
                "   - Takes the report from first chain\n",
                "   - Automatically fills `{topic}` variable (from first chain output)\n",
                "   - Summarizes the report into key points\n",
                "   - Result: Concise summary\n",
                "\n",
                "3. **Complete Pipeline**: `prompt1 | model | parser | prompt2 | model | parser`\n",
                "   - All steps execute in order\n",
                "   - No manual variable passing\n",
                "   - Output of parser 1 → Input to prompt2\n",
                "   - Final output is the summary\n",
                "\n",
                "### How Data Flows\n",
                "\n",
                "```\n",
                "Input: {'topic': 'Agentic AI'}\n",
                "         ↓\n",
                "    prompt1 formats: \"Generate a Detailed report about Agentic AI.\"\n",
                "         ↓\n",
                "    model generates: [Full AI explanation]\n",
                "         ↓\n",
                "    parser extracts: \"The detailed report...\"\n",
                "         ↓\n",
                "    prompt2 formats: \"Summarize this report: The detailed report...\"\n",
                "         ↓\n",
                "    model generates: [Summary of report]\n",
                "         ↓\n",
                "    parser extracts: \"Key points: 1. ... 2. ...\"\n",
                "         ↓\n",
                "    Final Output\n",
                "```\n",
                "\n",
                "### Best Practices for Sequential Chains\n",
                "\n",
                "✅ **DO:**\n",
                "- Test each component separately first\n",
                "- Use clear variable names that match template placeholders\n",
                "- Add intermediate parsing steps if needed\n",
                "- Log outputs at each step for debugging\n",
                "\n",
                "❌ **DON'T:**\n",
                "- Skip testing individual steps\n",
                "- Create overly long chains (5+ steps) without breaks\n",
                "- Use incompatible prompts (prompt output format ≠ next prompt input format)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0672cb37",
            "metadata": {},
            "outputs": [],
            "source": [
                "# imports\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
                "from langchain_core.runnables import RunnableParallel, RunnableBranch, RunnableLambda\n",
                "from pydantic import BaseModel, Field\n",
                "from typing import Literal\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "# Models\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"deepseek-ai/DeepSeek-V3.2\",\n",
                "    task=\"text-generation\")\n",
                "model = ChatHuggingFace(llm = llm)\n",
                "\n",
                "llm2 = HuggingFaceEndpoint(\n",
                "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.7)\n",
                "model2 = ChatHuggingFace(llm = llm2)\n",
                "\n",
                "#parsers\n",
                "parser = StrOutputParser()\n",
                "\n",
                "class Feedback(BaseModel):\n",
                "\n",
                "    sentiment : Literal['positive', 'negative'] = Field(description='Give the sentiment of the feedback')\n",
                "# since it is a llm we dont have control on its responce so we need to structure the output using Pydantic output parser to get controled and consistent output.\n",
                "parser2 = PydanticOutputParser(pydantic_object=Feedback)\n",
                "\n",
                "# review\n",
                "review = '''WI got a defective ASUS laptop — it would shut down during normal use, the screen was flickering, it would hang, and I couldn’t close apps from the taskbar. I complained to Amazon customer care within 3 days, while it was still under the replacement period. They told me to contact ASUS support.\n",
                "\n",
                "I spoke with ASUS customer care, and they sent a technician. He checked and confirmed the problem, then said Windows would need to be reinstalled. I told him that this is a new product, and it shouldn’t arrive defective — I wanted a replacement, not a repair. He wrote down the issue in a letter and said he’d check with the company and get back to me.\n",
                "\n",
                "Two days passed with no response. When I called again, they said the replacement request was cancelled. I contacted customer care again, and they sent another technician. He submitted a replacement request and took a copy of the invoice.\n",
                "\n",
                "Now, for a replacement, a DOA (Dead on Arrival) letter is required, which is made using the details from the invoice. But my invoice didn’t have the laptop’s serial number. Because of that, ASUS refused to issue the DOA letter, saying you needed to talk to Amazon.\n",
                "\n",
                "After talking to Amazon 5–7 times, they kept saying their policy doesn’t allow adding a serial number to the invoice. ASUS said they can’t make the DOA letter without it. Later, I gave them the warranty slip, and after 2–3 days they somehow managed to create the letter.\n",
                "\n",
                "But when I tried to submit the replacement request on Amazon, they said replacement isn’t available at all — it won’t happen. They told me to contact the manufacturer or seller. The seller’s phone didn’t even connect. ASUS then said it’s Amazon’s issue, not theirs.\n",
                "\n",
                "So now both of them are refusing to take responsibility, and the replacement just isn’t happening.\n",
                "\n",
                "I recieved the defective laptop on 27 September , and today is 17 october and still nothing has changed.'''\n",
                "\n",
                "# get detailed summary of review\n",
                "prompt = PromptTemplate(\n",
                "    template='''You are a customer experience analyst.\n",
                "\n",
                "Summarize the following customer review clearly and objectively.\n",
                "\n",
                "Guidelines:\n",
                "\n",
                "Identify whether the review is positive, negative, or mixed\n",
                "\n",
                "Highlight the main points the customer mentioned\n",
                "\n",
                "Do NOT add assumptions or extra information\n",
                "\n",
                "Keep it short and factual (2–4 bullet points or 2–3 lines)\n",
                "\n",
                "Customer Review:\n",
                "{feedback}''',\n",
                "    input_variables=['feedback'])\n",
                "\n",
                "# Get positive or negative\n",
                "prompt1 = PromptTemplate(\n",
                "    template=' Classify the sentiment of the following feedback text into possitive or negative:\\n {feedback}. \\n {format_instruction}',\n",
                "    input_variables=['feedback'],\n",
                "    partial_variables={'format_instruction': parser2.get_format_instructions()})\n",
                "\n",
                "# Responce for positive review\n",
                "prompt2 = PromptTemplate(\n",
                "    template='''You are a customer support manager for a professional company.\n",
                "\n",
                "Write a calm, polite, and empathetic response to the following negative customer review.\n",
                "\n",
                "Guidelines:\n",
                "\n",
                "Acknowledge the customer’s concern clearly\n",
                "\n",
                "Apologize where appropriate (without admitting legal fault)\n",
                "\n",
                "Offer a solution or next step\n",
                "\n",
                "Keep the tone respectful and professional\n",
                "\n",
                "Do not argue or sound defensive\n",
                "\n",
                "Keep it concise but helpful\n",
                "\n",
                "Customer Review:\n",
                "{feedback}\n",
                "''',\n",
                "    input_variables=['feedback'])\n",
                "\n",
                "# Responce for negative review\n",
                "prompt3 = PromptTemplate(\n",
                "    template='''You are a customer support manager for a professional company.\n",
                "\n",
                "Write a warm, appreciative, and professional response to the following positive customer review.\n",
                "\n",
                "Guidelines:\n",
                "\n",
                "Thank the customer genuinely\n",
                "\n",
                "Acknowledge what they liked specifically\n",
                "\n",
                "Reinforce the company’s commitment to quality/service\n",
                "\n",
                "Keep the tone friendly but professional\n",
                "\n",
                "Keep it concise\n",
                "\n",
                "Customer Review:\n",
                "{feedback}\n",
                "''',\n",
                "    input_variables=['feedback'])\n",
                "\n",
                "# summary of review\n",
                "sentiment = prompt | model | parser\n",
                "review_summary = sentiment.invoke({'feedback': review})\n",
                "\n",
                "# classify positive or negaive\n",
                "classifier_chain = prompt1 | model | parser2\n",
                "\n",
                "# conditional runnable for positive or negative responce\n",
                "branched_chain = RunnableBranch(\n",
                "    (lambda x: x.sentiment == 'positive', prompt3 | model2 | parser),\n",
                "    (lambda x: x.sentiment == 'negative', prompt2 | model2 | parser),\n",
                "    RunnableLambda(lambda x: ' we could not find sentiment'))\n",
                "\n",
                "# final chain for responce\n",
                "chain = classifier_chain | branched_chain\n",
                "result = chain.invoke({'feedback': review})\n",
                "\n",
                "# Output\n",
                "print('--> sentiment:\\n',review_summary)\n",
                "print()\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-4",
            "metadata": {},
            "source": [
                "### Code Explanation: Conditional Chain\n",
                "\n",
                "1. **Define Classification Prompt**:\n",
                "   - `prompt1`: Classifies sentiment as positive/negative\n",
                "   - Uses `PydanticOutputParser` to enforce structured output\n",
                "   - Returns `Feedback` object with sentiment field\n",
                "\n",
                "2. **Create Classifier Chain**:\n",
                "   - `classifier_chain = prompt1 | model | parser2`\n",
                "   - Output: Pydantic object with `.sentiment` attribute\n",
                "\n",
                "3. **Define Response Prompts**:\n",
                "   - `prompt2`: Response template for negative sentiment\n",
                "   - `prompt3`: Response template for positive sentiment\n",
                "   - Each tailored to specific sentiment\n",
                "\n",
                "4. **Create RunnableBranch**:\n",
                "   ```python\n",
                "   branched_chain = RunnableBranch(\n",
                "       (lambda x: x.sentiment == 'positive', prompt3 | model2 | parser),\n",
                "       (lambda x: x.sentiment == 'negative', prompt2 | model2 | parser),\n",
                "       RunnableLambda(lambda x: 'sentiment not found')\n",
                "   )\n",
                "   ```\n",
                "   - Condition: Check `x.sentiment` value\n",
                "   - If positive → Use prompt3 (appreciative tone)\n",
                "   - If negative → Use prompt2 (empathetic tone)\n",
                "   - Fallback: Return error message\n",
                "\n",
                "5. **Complete Pipeline**:\n",
                "   - `chain = classifier_chain | branched_chain`\n",
                "   - First classify sentiment\n",
                "   - Then route to appropriate response template\n",
                "   - Generate customized response\n",
                "\n",
                "### How Conditional Logic Works\n",
                "\n",
                "```\n",
                "Customer Review Input\n",
                "         ↓\n",
                "[Classify Sentiment] → Feedback(sentiment='negative')\n",
                "         ↓\n",
                "[Evaluate Condition] → x.sentiment == 'positive'? NO\n",
                "                    → x.sentiment == 'negative'? YES ✓\n",
                "         ↓\n",
                "[Execute Negative Response Chain]\n",
                "  prompt2 | model | parser\n",
                "         ↓\n",
                "[Generate Empathetic Response]\n",
                "\"We understand your frustration...\"\n",
                "         ↓\n",
                "Final Response Output\n",
                "```\n",
                "\n",
                "### Best Practices for Conditional Chains\n",
                "\n",
                "✅ **DO:**\n",
                "- Use structured output parsers (Pydantic) for reliable classification\n",
                "- Have clear, distinct conditions\n",
                "- Provide meaningful fallback chains\n",
                "- Test all branches thoroughly\n",
                "- Use lambda for simple conditions, functions for complex\n",
                "\n",
                "❌ **DON'T:**\n",
                "- Use overlapping conditions\n",
                "- Forget to handle edge cases\n",
                "- Make conditions overly complex\n",
                "- Skip testing fallback paths\n",
                "- Create too many branches (>5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "321dfbd9",
            "metadata": {},
            "source": [
                "## Summary: Chains in LangChain\n",
                "\n",
                "### Chain Types Quick Reference\n",
                "\n",
                "| Chain Type | Pattern | Best For | Complexity |\n",
                "|-----------|---------|----------|-----------|\n",
                "| **Simple** | `prompt \\| model \\| parser` | Single transformation | Low |\n",
                "| **Sequential** | `chain1 \\| chain2 \\| chain3` | Multi-step workflows | Medium |\n",
                "| **Parallel** | `RunnableParallel({...})` | Multi-aspect analysis | Medium |\n",
                "| **Conditional** | `RunnableBranch(...)` | Dynamic routing | Medium-High |\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **LCEL is Powerful**: The pipe operator `|` makes composition simple and readable\n",
                "2. **Sequential**: Output automatically becomes input (variable mapping)\n",
                "3. **Parallel**: Run independent tasks simultaneously for speed\n",
                "4. **Conditional**: Route execution based on data properties\n",
                "5. **Composability**: Combine all types for complex workflows\n",
                "\n",
                "### When to Use Each Chain Type\n",
                "\n",
                "| Requirement | Chain Type |\n",
                "|------------|-----------|\n",
                "| Generate text → Parse it | Simple Chain |\n",
                "| Generate → Summarize → Polish | Sequential Chain |\n",
                "| Get summary + notes + quiz | Parallel Chain |\n",
                "| Different response per sentiment | Conditional Chain |\n",
                "| All of the above | Combined approach |\n",
                "\n",
                "### Common Patterns\n",
                "\n",
                "```python\n",
                "# Pattern 1: Simple chain\n",
                "chain = prompt | model | parser\n",
                "\n",
                "# Pattern 2: Sequential chain\n",
                "chain = (prompt1 | model | parser) | (prompt2 | model | parser)\n",
                "\n",
                "# Pattern 3: Parallel chain\n",
                "parallel = RunnableParallel({'output1': chain1, 'output2': chain2})\n",
                "final_chain = parallel | merge_prompt | model | parser\n",
                "\n",
                "# Pattern 4: Conditional chain\n",
                "branched = RunnableBranch(\n",
                "    (lambda x: condition(x), chain_if_true),\n",
                "    default_chain\n",
                ")\n",
                "final_chain = classifier | branched\n",
                "\n",
                "# Pattern 5: Complete workflow\n",
                "full_chain = (prompt | model | parser) | parallel | branched\n",
                "```\n",
                "\n",
                "### Error Handling Tips\n",
                "\n",
                "✅ **DO:**\n",
                "- Test each component independently\n",
                "- Add validation prompts\n",
                "- Use try-except in invoke calls\n",
                "- Log intermediate outputs\n",
                "- Handle timeouts and API errors\n",
                "\n",
                "### Performance Optimization\n",
                "\n",
                "- Use parallel chains for independent tasks\n",
                "- Set appropriate temperature values\n",
                "- Cache prompt templates if reused\n",
                "- Use faster models for simple tasks\n",
                "- Monitor API rate limits\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "After mastering chains:\n",
                "1. **Memory**: Add conversation history to chains\n",
                "2. **Agents**: Create autonomous decision-making systems\n",
                "3. **RAG**: Combine chains with document retrieval\n",
                "4. **Custom Runnables**: Build specialized components\n",
                "5. **Production**: Deploy chains with error handling and monitoring\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
