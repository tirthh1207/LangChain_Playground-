{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ce0e3848",
            "metadata": {},
            "source": [
                "### Outputparser\n",
                "- Output Parsers in LangChain are responsible for taking the raw output of an LLM and transforming it into a more suitable format.\n",
                "\n",
                "- Here is a breakdown of their role:\n",
                "\n",
                "    - Structure from Chaos: LLMs natively output text (strings). But in programming, we often need structured data like lists, dictionaries, or database records. Output Parsers bridge this gap.\n",
                "\n",
                "    - Two Main Jobs:\n",
                "        - Formatting Instructions: They can inject instructions into the prompt telling the LLM generally how to format its response (e.g., \"Return valid JSON\").\n",
                "        - Parsing: They take the final text response and convert it into a Python object (e.g., parsing a stringified JSON into a real Python dict).\n",
                "\n",
                "- Common Types:\n",
                "    - StrOutputParser: The simplest one. It just extracts the text content from the message object, converting AIMessage(content=\"Hello\") $\\to$ \"Hello\".\n",
                "    - JsonOutputParser: Ensures the output is valid JSON and converts it into a Python Dictionary.\n",
                "    - PydanticOutputParser: The most powerful one. It uses a Python class (Pydantic model) to define exactly what fields, data types, and validation rules logic the output must follow.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "45a2fead",
            "metadata": {},
            "source": [
                "### Theory: StrOutputParser and Chaining\n",
                "\n",
                "1. **StrOutputParser**: \n",
                "   - This is the simplest output parser in LangChain. \n",
                "   - LLMs often return complex objects (like `AIMessage` containing content, metadata, etc.). \n",
                "   - `StrOutputParser` extracts just the string content (the text body) from the response, converting it into a standard Python string. This makes it ready to be passed into the next step of a chain.\n",
                "\n",
                "2. **Chain of Thought (Multi-step Logic)**:\n",
                "   - The chain `template1 | model | parser | template2 | model | parser` demonstrates a pipeline.\n",
                "   - **Step 1**: `template1` generates a prompt asking for a \"detailed report\".\n",
                "   - **Step 2**: `model` generates the report (as an `AIMessage`).\n",
                "   - **Step 3**: `parser` converts the `AIMessage` to a pure string.\n",
                "   - **Step 4**: This string text is automatically passed as input to `template2` (filling the `{text}` variable) which asks for a summary.\n",
                "   - **Step 5 & 6**: The model generates the summary, and the final parser cleans it up for printing.\n",
                "\n",
                "This pattern is crucial for complex workflows where the output of one step becomes the input of the next."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "78dd9e29",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "### Point-wise Summary of the Beginner's Guide to Large Language Models (LLMs)\n",
                        "\n",
                        "1. **Introduction**:\n",
                        "   - Large Language Models (LLMs) are AI models designed to process and generate human-like text.\n",
                        "   - They are trained on vast amounts of text data, enabling them to understand and generate text across multiple languages and contexts.\n",
                        "   - Applications include customer service, content creation, and language translation.\n",
                        "\n",
                        "2. **Definition of LLMs**:\n",
                        "   - LLMs predict the next word in a sequence of text based on patterns learned from large datasets.\n",
                        "   - The goal is to generate coherent and contextually relevant text.\n",
                        "\n",
                        "3. **Key Features of LLMs**:\n",
                        "   - **Massive Size**: Contain billions or trillions of parameters.\n",
                        "   - **Contextual Understanding**: Can understand the context of sentences and conversations.\n",
                        "   - **Multilingual Support**: Capable of processing and generating text in multiple languages.\n",
                        "   - **Fine-Tuning**: Can be adapted for specific tasks through fine-tuning.\n",
                        "\n",
                        "4. **How LLMs Work**:\n",
                        "   - **Training Process**:\n",
                        "     - **Data Collection**: Involves large datasets of text from various sources.\n",
                        "     - **Model Architecture**: Typically uses Transformer models.\n",
                        "     - **Training**: Uses backpropagation to adjust parameters and minimize differences between predicted and actual text.\n",
                        "     - **Evaluation**: Measures performance on a separate dataset.\n",
                        "   - **Inference Process**:\n",
                        "     - **Input**: Takes a sequence of text as input.\n",
                        "     - **Prediction**: Generates the next word based on learned patterns.\n"
                    ]
                }
            ],
            "source": [
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
                "    task=\"text-generation\",\n",
                "    temperature=0.3)\n",
                "model = ChatHuggingFace(llm=llm)\n",
                "\n",
                "# 1. StrOutputParser example code-------\n",
                "\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "# Prompt 1 → detailed report\n",
                "template1 = PromptTemplate(\n",
                "    template=\"\"\"\n",
                "You are a helpful AI tutor.\n",
                "Write a beginner friendly detailed report on {topic}.\n",
                "\"\"\",\n",
                "    input_variables=[\"topic\"]\n",
                ")\n",
                "\n",
                "# Prompt 2 → summary\n",
                "template2 = PromptTemplate(\n",
                "    template=\"\"\"\n",
                "Write a short point-wise summary of the following text:\n",
                "{text}\n",
                "\"\"\",\n",
                "    input_variables=[\"text\"]\n",
                ")\n",
                "\n",
                "parser = StrOutputParser()\n",
                "\n",
                "chain = template1 | model | parser | template2 | model | parser\n",
                "\n",
                "result = chain.invoke({\"topic\": \"LLM\"})\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cf10b609",
            "metadata": {},
            "source": [
                "### Theory: JsonOutputParser and Structure Enforcement\n",
                "\n",
                "1. **JsonOutputParser**: \n",
                "   - LLMs natively generate text. Getting them to generate structured data (like JSON) can be tricky.\n",
                "   - This parser does two things: \n",
                "      - Provide instructions to the model on how to format the JSON.\n",
                "      - Parse the resulting string into a Python Dictionary `dict`.\n",
                "\n",
                "2. **Injecting Instructions**:\n",
                "   - `parser.get_format_instructions()` returns a string containing instructions like \"Return a JSON object with keys...\"\n",
                "   - `partial_variables`: This feature allows us to pre-fill variables in the prompt template. Here, we inject `{format_instruction}` automatically so we don't have to type it out manually every time.\n",
                "   - Note: The prompt explicitly includes `{format_instruction}` to ensure the model \"sees\" these rules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1349ddee",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'name': 'Sarah Mitchell', 'age': 32, 'address': '742 Evergreen Terrace, Springfield, IL 62701'}\n"
                    ]
                }
            ],
            "source": [
                "# JsonOutputParser\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_core.output_parsers import JsonOutputParser\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "#model \n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"MiniMaxAI/MiniMax-M2.1\",\n",
                "    task=\"text-generation\")\n",
                "model = ChatHuggingFace(llm = llm)\n",
                "\n",
                "#OutputParser\n",
                "parser = JsonOutputParser()\n",
                "\n",
                "#template\n",
                "template = PromptTemplate(\n",
                "    template=\"give me a name, age, address of a fictional character.\\n {format_instruction}\",\n",
                "    input_variables=[],\n",
                "    partial_variables={\"format_instruction\": parser.get_format_instructions()}\n",
                ")\n",
                "\n",
                "chain = template | model | parser\n",
                "result = chain.invoke({})\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "expl-2",
            "metadata": {},
            "source": [
                "### Theory: PydanticOutputParser and Data Validation\n",
                "\n",
                "1. **PydanticOutputParser**: \n",
                "   - This is the most robust parsing method available in LangChain for structured data.\n",
                "   - It uses the **Pydantic** library, which allows you to define strict schemas (Blueprints) for your data.\n",
                "\n",
                "2. **Schema Definition**:\n",
                "   - `class Person(BaseModel)`: We define a class that represents the structure we want.\n",
                "   - `Field(description=...)`: We provide descriptions for each field. The LLM uses these descriptions to understand *what* to put in that field.\n",
                "   - **Validation**: Notice `age : int = Field(gt=18)`. This enforces rules (e.g., age must be greater than 18). If the LLM generates an age of 10, the parser can catch this error (and in advanced setups, auto-correction chains can ask the LLM to fix it).\n",
                "\n",
                "3. **Type Safety**:\n",
                "   - The output `result` is not just a dictionary, but an instance of the `Person` class (or equivalent object). This is critical for building reliable applications where you need to be 100% sure that the output data matches your software's expected types."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "675886a1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "name='Rajiv Menon' age=28 address='24, Gandhi Nagar, Chennai, Tamil Nadu, India'\n"
                    ]
                }
            ],
            "source": [
                "# PydenticOutputParser\n",
                "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_core.output_parsers import PydanticOutputParser\n",
                "from pydantic import BaseModel, Field\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "#model \n",
                "llm = HuggingFaceEndpoint(\n",
                "    repo_id=\"MiniMaxAI/MiniMax-M2.1\",\n",
                "    task=\"text-generation\")\n",
                "model = ChatHuggingFace(llm = llm)\n",
                "\n",
                "class Person(BaseModel):\n",
                "    name: str = Field(description='Persons name')\n",
                "    age : int = Field(gt=18, description=\"Age of Person\")\n",
                "    address : str = Field(description=\"Place where person belongs to\")\n",
                "\n",
                "\n",
                "parser = PydanticOutputParser(pydantic_object=Person)\n",
                "\n",
                "template = PromptTemplate(\n",
                "    template= \"get me name, age, address of a fictional {type} Person.\\n {format_instruction}\",\n",
                "    input_variables=[\"type\"],\n",
                "    partial_variables={\"format_instruction\": parser.get_format_instructions()}\n",
                ")\n",
                "\n",
                "chain = template | model | parser\n",
                "result = chain.invoke({\"type\": \"indian\"})\n",
                "print(result)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
